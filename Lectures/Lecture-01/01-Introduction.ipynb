{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction and Overview\n",
    "__Quantitative Big Imaging__ ETHZ: 227-0966-00L\n",
    "\n",
    "<p style=\"font-size:1em;\">February 24, 2022</p>\n",
    "<br /><br />\n",
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Anders Kaestner</p>  \n",
    "<p style=\"font-size:1em;\">Laboratory for Neutron Scattering and Imaging<br />Paul Scherrer Institut</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "\n",
    "- About the course\n",
    "- Motivating the use of quantitive methods in imaging\n",
    "- What is an image?\n",
    "- Where do images come from?\n",
    "- Science and Reproducibility\n",
    "- Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We need some python modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Python is a modular scripting language with limited functionality. Features are added through modules that are imported.\n",
    "These are the modules that are needed for this lecture. Please run this cell before you start using the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage import convolve\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "from itertools import product\n",
    "import os\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About the course\n",
    "\n",
    "- Who are we?\n",
    "- Who are you?\n",
    " - What is expected?\n",
    "- __Why does this class exist?__\n",
    " - Collection\n",
    " - Changing computing (Parallel / Cloud)\n",
    " - Course outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Who are we?\n",
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Anders Kaestner, PhD</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Anders Kaestner__\n",
    "\n",
    "You will hear me a lot during this course. I am the lecturer and I will also support you with problems during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column23\">\n",
    "      \n",
    "- __Beamline scientist__ at the ICON Beamline at the SINQ (Neutron Source) at Paul Scherrer Institute\n",
    "    - __Lecturer__ at ETH Zurich\n",
    "- __Algorithm developer__ Varian Medical Systems, Baden-Daettwil\n",
    "- __Post Doc__ at ETH Zurich, Inst for Terrestial Ecology\n",
    "- __PhD__ at Chalmers Institute of Technology, Sweden, Signal processing\n",
    "\n",
    "\n",
    "anders.kaestner@psi.ch\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "    <img src=\"figures/anders.jpeg\" style=\"height:350px\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Stefano van Gogh</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Stefano van Gogh__\n",
    "\n",
    "Will help you during the exercise sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column23\">\n",
    "      \n",
    "- __PhD Student__ in the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute\n",
    "- Teaching assistant\n",
    "\n",
    "Will support remotely the first part.\n",
    "\n",
    "stefano.van-gogh@psi.ch\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "     <img src=\"https://www.psi.ch/sites/default/files/styles/primer_teaser_square_scale/public/2019-06/picture.jpg?itok=t9wRh5Yb\" style=\"height:350px\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Who are you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This course is targeting a wide range of students with different levels of experience. In the table you'll see were students came from in previos years. Some have a technical background others are merely producing images in the line of their project and have never seen much more than photoshop and similar programs for processing image data. Using some kind of programming is nescessary to perform quantitative image analysis on large data sets. A single or a few images can easily be handled with interactive software, but taking it beyond that is hard without writing some lines of code. \n",
    "\n",
    "Now, some of you have little to no programming experience while others have been programming since they got their first computer in the hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='figures/background_skills.svg' style=\"height:700px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So how will this ever work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now you maybe start to get worried! The purpose of this course is not to teach you programming but rather to provide you with a bag full recipes that you can use in your projects. Most of these recipes are just a list of the commands from different python moduls that you need to perform your analysis. A side-effect will probably be that you learn one or two programming tricks on the way.\n",
    "\n",
    "In the lectures, there will be small code pieces on the slides. Some of these are there to illustrate how an operation works, while other parts are there for the nice presentation of the results (this is mostly the second half of the code cell). Presenting the results is important. In the end, you want to show your results to the scinetific community. So even though the plotting clutters the slide, there is something to learn there as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Adaptive assignments__\n",
    "\n",
    "- Conceptual, graphical assignments with practical examples\n",
    "  - Emphasis on chosing correct steps and understanding workflow\n",
    "\n",
    "\n",
    "\n",
    "- Opportunities to create custom implementations, and perform more complicated analysis on larger datasets if interested\n",
    "  - Emphasis on performance, customizing analysis, and scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Course Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The practical part of the course has two parts. None of these are mandatory, but they will help you to better understand use the material you have learnt in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercises\n",
    "- Usually 1 set per lecture\n",
    "- Optional (but recommended!)\n",
    "- Level\n",
    "    - *Easy* - Jupyter notebooks are prepared for the exercises\n",
    "    - *Advanced* - Writing Python, Java, C++, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The exercises are prepared in a way that you learn step by step what you need to do and guids you through the problems. We will be using jupyter notebooks for the lectures. This is a very common way to work with image data these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the optional science projects you will have to opportunity to test what you have learned during the course on real problems. This is the place for your creativity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The project is optional (but strongly recommended)\n",
    "- A small image processing project\n",
    "- Can be related to you Master or PhD project\n",
    "- You will get input and ideas for your own projects\n",
    "- You will get hands on experience on the techniques you learn here\n",
    "- Can be used as discussion base for your exam\n",
    "\n",
    "\n",
    "**Goal**\n",
    "- Applying Techniques to answer scientific question!\n",
    "- Ideally use on a topic relevant for your current project, thesis, or personal activities or choose from one of ours (will be online, soon)\n",
    "- Presentation: (approach, analysis, and results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Course Overview\n",
    "\n",
    "| \\# |Topic| Date| Title | Description |\n",
    "|:---:|:---:|:---|:---|:---|\n",
    "| 1 | __Introduction__ | 24th February| Introduction and Workflows | Basic overview of the course, introduction to ...|\n",
    "| 2 | __Data__         | 3th March | Image Enhancement |\tOverview of what techniques are available for ...|\n",
    "| 3 |                  | 10th March | Ground Truth: Building and Augmenting Datasets | Examples of large datasets, how they were buil... |\n",
    "| 4 |__Segmentation__  | 17th March | Basic Segmentation, Discrete Binary Structures | How to convert images into structures, startin... |\t\n",
    "| 5 |                  | 24th March\t   | Advanced Segmentation\t| More advanced techniques for extracting struct... |\n",
    "| 6 |                  | 31st March      | Supervised Problems and Segmentation\t| More advanced techniques for extracting struct...|\n",
    "| 7 | __Analysis__     | 7th April\t| Analyzing Single Objects, Shape, and Texture |\tThe analysis and characterization of single st...|\n",
    "|8 |                  | 14th April\t| Statistics, Prediction, and Reproducibility\t| What techniques are available to analyze more ...|\n",
    "| - |                  | 21th April  | Easter break | Search for eggs|\n",
    "| 9 |  __Big Imaging__                | 28th April\t| Dynamic Experiments\t| Performing tracking and registration in dynami...|\n",
    "| 10 |   | 5th May\t| Imaging with multiple modalities | Combining information from different sources |\n",
    "|                  \n",
    "| 11 |                  | 12th May   | Scaling Up / Big Data\t|Performing large scale analyses on clusters |\t\n",
    "| 12 |                  | 19th May   | Project | Project consultation |\n",
    "|  -                | 26th May\t| Ascension | Enjoy a lovely early summers day |\n",
    "| 13 |__Wrapping up__   | 2nd June   | Project Presentations |\tYou present your projects|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading Material\n",
    "\n",
    "- Some book on image processing with python (to be updated)\n",
    "- Cloud Computing\n",
    " - [The Case for Energy-Proportional Computing](http://www-inst.eecs.berkeley.edu/~cs61c/sp14/) _Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007_\n",
    " - [Concurrency](http://www.gotw.ca/publications/concurrency-ddj.htm)\n",
    "- Reproducibility\n",
    " - [Trouble at the lab](http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble) _Scientists like to think of science as self-correcting. To an alarming degree, it is not_\n",
    " - [Why is reproducible research important?](http://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/) _The Real Reason Reproducible Research is Important_\n",
    " - [Science Code Manifesto](http://software-carpentry.org/blog/2011/10/the-science-code-manifestos-five-cs.html)\n",
    " - [Reproducible Research Class](https://www.coursera.org/course/repdata) @ Johns Hopkins University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Literature / Useful References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These are books that are useful in many of the lectures. In particular the Image processing hand book by John Russ shows you an overview of typical image processing techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)\n",
    "    - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN)\n",
    "- Jean Claude, Morphometry with R\n",
    "    - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation - You have data!\n",
    "\n",
    "## Imaging experiments produce a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Working with imaging techniques you will get a lot of images that shows the sample in the eye of the technique you are using. The experiment were you acquire these images is only a small fraction of the complete workflow from idea to the final scientific publiction. The amout of data can also be overwhelming for many scientist with the consequence that the data is never analyzed properly, and then also not published in the way it really deserves. \n",
    "```{figure} figures/experimentdata.pdf\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "A typical imaging experiment produces large amounts of data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/experimentdata.svg\" />\n",
    "\n",
    "A typical imaging experiment produce **gigabytes** and even **terabytes** of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation - We want to understand our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the beginning you only have data, but you want statistics to support your hypothesis. To achieve this you need to apply a series of operation steps. With each step you come closer to the answer you are seeking from the data. In the plot here you see that the initial processing steps may not provide much more understanding, but they are needed to prepare your data for the analysis.\n",
    "```{figure} figures/DataKnowledge.pdf\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "Your understanding of the data increases with each processing step.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/DataKnowledge.svg\" style=\"height:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation - how to proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now is the question how to proceed towards a working analysis workflow that results in repeatable analyses for your data.\n",
    "```{figure} figures/crazyworkflow.png\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "A crazy unstructured and unclear work flow to analyze images from your experiment.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/crazyworkflow.png\">\n",
    "\n",
    "- To understand what, why and how from the moment an image is produced until it is finished (published, used in a report, …)\n",
    "- To learn how to go from one analysis on one image to 10, 100, or 1000 images (without working 10, 100, or 1000X harder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The experiment life cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we have seen that there is a wish to obtain data at high rates and that there are technological solutions to provide this. The remainging part to develop is the post processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. __Experimental Design__ finding the right technique, picking the right dyes and samples has stayed relatively consistent, better techniques lead to more demanding scientists.\n",
    "\n",
    "2. __Measurements__ the actual acquisition speed of the data has increased wildly due to better detectors, parallel measurement, and new higher intensity sources\n",
    "\n",
    "3. __Management__ storing, backing up, setting up databases, these processes have become easier and more automated as data magnitudes have increased\n",
    "\n",
    "4. __Post Processing__ this portion has is the most time-consuming and difficult and has seen minimal improvements over the last years\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The post processing is the least trivial part to generalize. The initial steps are often posible to generalize as these are operations that all types of imaging experiments need to go through. When it comes to the experiment specific analysis the degree of generalization decreases and the scientists are left to develop their own procedure to extract the quantitative information from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "time-figure, fig.width=12, fig.height=8",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How is time used during the experiment life cycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "With the development of faster acquisision systems there has been a change in the ratio between \n",
    "- Exeperiment design and preparation\n",
    "- Measurements\n",
    "- Data management\n",
    "- and post processing\n",
    "\n",
    "over the years. This in particular the case for X-ray imaging where the flux is high and the acquisition is limited by the detector technology. In other modalities, where the measurement is flux limited we see a different distribution. \n",
    "\n",
    "```{figure} figures/qmia-014.png\n",
    "---\n",
    "scale: 50%\n",
    "--- \n",
    "The ratio of how much time is spent on different tasks during the lifecycle of an imaging experiment.\n",
    "```\n",
    "\n",
    "What also increases the post processing time is that the experiments have become more complicated over the years. Twenty years ago, it was sufficient to show qualitative information a beautify volume rendering or a movie of the sample. Meanwhile, it has become a requirement that you provide quantitative results from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "time-figure, fig.width=12, fig.height=8",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/qmia-014.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## High acquisition rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The trend in imaging is that experimentalist want to follow faster and faster processes. This wish can be supported the technical development of new detectors that provide very high acqisition rates. Here, we can also see that some cameras are able to produce more data than is uploaded per day on facebook and instagram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Detectors are getting bigger and faster constantly\n",
    "- Todays detectors are really fast\n",
    "    - 2560 x 2160 images @ 1500+ times a second = 8GB/s\n",
    "- Matlab / Avizo / Python / … are saturated after 60 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Many of the analysis platforms are already overwhelmed with handling the data rates produced by typical detector systems at imaging instrument. This restriction is partly due to hardware limitations. The memory is to small, hard drives are not sufficiently fast. The other side of the problem is that these tools are not prepared to work with large data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- A single camera\n",
    "    - [More information per day than Facebook](http://news.cnet.com/8301-1023_3-57498531-93/facebook-processes-more-than-500-tb-of-data-daily/)\n",
    "    - [Three times as many images per second as Instagram](http://techcrunch.com/2013/01/17/instagram-reports-90m-monthly-active-users-40m-photos-per-day-and-8500-likes-per-second/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different sources of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Images are produced by many different detectors and in some cases they are even the output from simulations. In the next sections we see some different imaging modalities and the data rates they produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### X-Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "X-ray imaging at syncrotron light sources produces very high frame rates thanks to the high brilliance of the source. Here are some examples of data rates from some instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - SRXTM images at (>1000fps) → 8GB/s\n",
    " - cSAXS diffraction patterns at 30GB/s\n",
    " - Nanoscopium Beamline, 10TB/day, 10-500GB file sizes\n",
    "\n",
    "### Optical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Optical imaging methods are more modest than the X-ray techniques, but still they produce data in the order of some hundred Mb per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - Light-sheet microscopy (see talk of Jeremy Freeman) produces images → 500MB/s\n",
    " - High-speed confocal images at (>200fps) → 78Mb/s\n",
    "\n",
    "### Personal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, we also take a look at cameras on the consumer market and see that these devices also produce relatively high data rates. This data must mostly be handled by normal household computers, which can be a challenging task..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- GoPro 4 Black - 60MB/s (3840 x 2160 x 30fps) for \\$600\n",
    "- [fps1000](https://www.kickstarter.com/projects/1623255426/fps1000-the-low-cost-high-frame-rate-camera) - 400MB/s (640 x 480 x 840 fps) for $400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling masses of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/data_trap_2x.png\n",
    "---\n",
    "scale: 50%\n",
    "--- \n",
    "New data will be produced once you start working with it.\n",
    "```\n",
    "```{figure} figures/DataProduced.pdf\n",
    "---\n",
    "scale: 50%\n",
    "--- \n",
    "A qualitative plot showing how you data will grow with different processing steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"figures/data_trap_2x.png\" style=\"height:600px\" />\n",
    "    <a href=\"https://xkcd.com/2582/\">From xkcd</a>\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"figures/DataProduced.svg\" style=\"height:600px\" />\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### So... how much is a TB, really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We have been talking about different data amounts of MB, GB, and TB. But, what does that really mean in reality? Let us explore what is a TB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If __you__ looked at one image with 1024 x 1024 pixels (1 Mpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we create one image with 1000x1000 pixels with random values form a uniform distribution [0,1] and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.random.uniform(size = (1024, 1024)), \n",
    "           cmap = 'viridis');\n",
    "plt.title('1k x 1k random image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "every second, it would take you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# assuming 16 bit images and a 'metric' terabyte\n",
    "OneTB     = 1e12\n",
    "ImageSize = 1000*1000*16/8\n",
    "hour      = 60*60\n",
    "\n",
    "time_per_tb = OneTB/ImageSize/hour\n",
    "print(\"{0:0.1f} hours to view a terabyte\".format(time_per_tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overwhelmed scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Providing quantitative statements about image data is often very hard. You can may manage to do it on a single images like the bone image below.\n",
    "\n",
    "```{figure} figures/bone-cells.png\n",
    "---\n",
    "---\n",
    "A slice image show bone cells. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You would like to know:\n",
    "\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "- Count how many cells are in the bone slice.        \n",
    "- Ignore the ones that are ‘too big’ or shaped ‘strangely’.       \n",
    "- Are there more on the right side or left side?       \n",
    "- Are the ones on the right or left bigger, top or bottom?\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "<img src=\"figures/bone-cells.png\" style=\"height:50%\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More overwhelmed scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Statistical analysis requires that you study many samples and not just a single one. The samples are also objects which requires 3D data instead of a single 2D slice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Many samples are needed:\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "- Do it all over again for 96 more samples\n",
    "- This time in 3D with 2000 slices instead of just one!\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <img src=\"figures/96-samples.png\" style=\"height:50%\" />\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/96-samples.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A collection of 96 volume images from different bone samples.\n",
    "``` \n",
    "Working with multiple 3D images is not feasible anymore to do manually. We need some kind of automated process to perform the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bring on the pain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The 96 samples only represented one of our cases in the study. Now, if we want to study different ages, healthy/diseased, etc, we need to add a sample batch for each case. Maybe we even need to increase the number of samples in each test group. With all these variations, we can easily end up in a thousand samples to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Great variations in the population   \n",
    "\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "- Now again with 1090 samples!\n",
    "- How to measure?\n",
    "- How to analyze?\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <img src=\"figures/1090-samples.png\" style=\"height:50%\"/> \n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/1090-samples.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A collection of 1090 bone samples. This is a massive task to analyze!\n",
    "```\n",
    "With so many samples we stand in front of a logistic problem to measure the data and once the data is there we have to analyze it. As a first step, we have to specify how to analyze these images to obtain results that may or may not support a hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It gets better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The metrics we specified in the previous example are easy to observe and also to measure. They are direct measurements of pixels and positions. What if we now want to make more complicated inquiries even. Now how do we categorize the images or collections of features using soft metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Those metrics were quantitative and could be easily visually extracted from the images\n",
    "- What happens if you have _softer_ metrics\n",
    "    - How aligned are these cells?\n",
    "    - Is the group on the left more or less aligned than the right?\n",
    "    - errr?\n",
    "  \n",
    "<img src=\"figures/alignment-figure.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/alignment-figure.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Close-up on different bone segments. How aligned are the cells in these images?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dynamic Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Many experiments are on top of the spatial dimensions also studies over time. This brings us 4D data sets to analyze. How are we supposed to handle this? Looking at the movie we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "- How many bubbles are here?\n",
    "- How fast are they moving?\n",
    "- Do they all move the same speed?\n",
    "- Do bigger bubbles move faster?\n",
    "- Do bubbles near the edge move slower?\n",
    "- Are they rearranging?\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <video controls loop src=\"movies/dk31_foam.mp4\" type=\"video/mp4\" height=\"450px\"></video>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bringing in the scientific method\n",
    "\n",
    "## What is the purpose?\n",
    "\n",
    "- Discover and validate new knowledge\n",
    "\n",
    "### How?\n",
    "- Use the scientific method as an approach to convince other people\n",
    "- Build on the results of others so we don't start from the beginning\n",
    "\n",
    "### Important Points\n",
    "- While __qualitative__ assessment is important, it is difficult to reliably produce and scale\n",
    "- __Quantitative__ analysis is far from perfect, but provides metrics which can be compared and regenerated by anyone\n",
    "\n",
    "<small>Inspired by: [imagej-pres](http://www.slideshare.net/CurtisRueden/imagej-and-the-scijava-software-stack)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Science and Imaging\n",
    "Images are great for qualitative analyses since our brains can quickly interpret them without large _programming_ investements.\n",
    "### Proper processing and quantitative analysis is however much more difficult with images.\n",
    " - If you measure a temperature, quantitative analysis is easy, $50K$.\n",
    " - If you measure an image it is much more difficult and much more prone to mistakes, subtle setup variations, and confusing analyses\n",
    "\n",
    "\n",
    "### Furthermore in image processing there is a plethora of tools available\n",
    "\n",
    "- Thousands of algorithms available\n",
    "- Thousands of tools\n",
    "- Many images require multi-step processing\n",
    "- Experimenting is time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/GoldPendant.png\n",
    "---\n",
    "---\n",
    "A realistic rendering of an ancient gold pendant.\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} figures/NailHistogram.png\n",
    "---\n",
    "---\n",
    "Material analysis of a rusty roman nail.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Nice rederings is mostly not sufficient for a publication\n",
    "- It is important to define how you want to use your images\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      <center>\n",
    "      <img src=\"figures/GoldPendant.png\" style=\"height:500px\"/>\n",
    "      </center>\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "      <center>\n",
    "        <img src=\"figures/NailHistogram.png\" style=\"height:500px\"/>\n",
    "          </center>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<table>\n",
    "    <td> </td>\n",
    "<td>\n",
    "</td>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Initial questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You need to ask yourself a number of questions before you start to plan an experiment that involves images. It is good to a at least have a plan about how you are going to analyze your images ones you have them. It is not always obvious how you get from the experiment idea, to performing the experiment and finally how you perform the analysis to obtain the information you are really interested in for your research project.\n",
    "\n",
    "```{figure} figures/WeWantThis.png\n",
    "---\n",
    "---\n",
    "Things a scientist wants to know from an experiment.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What am I looking for?\n",
    "- Is my original question specific or too general?\n",
    "- How can I plan my experiment to make the analysis easier?\n",
    "- How can I transfer the original question into the vocabulary of image analysis?\n",
    "- How much a priori information about the sample do I have? Can it be used for the analysis?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"figures/WeWantThis.png\" style=\"height:500px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is the purpose of the experiment\n",
    "- 3D visualization\n",
    "- Sample characterization\n",
    "- Model process parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What can we measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There is a leap between what you really want to know and what we actually can measure in the images.\n",
    "\n",
    "```{figure} figures/WeCanMeasureThis.png\n",
    "---\n",
    "---\n",
    "These are things we can measure in images.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "|Quantification from gray values| Quantification from shapes |\n",
    "|:---|:---|\n",
    "| Material composition | Identify and count items |\n",
    "| Material transport | Measure volume |\n",
    "|| Characterize shape |\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"figures/WeCanMeasureThis.png\" style=\"height:500px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why quantitative?\n",
    "\n",
    "### Human eyes have issues\n",
    "\n",
    "Which center square seems brighter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.align='center'",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 3)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "img_a = 25*np.ones((3,3))\n",
    "img_b = np.ones((3,3))*75\n",
    "img_a[1,1] = 50\n",
    "img_b[1,1] = 50\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "ax1.matshow(img_a, vmin = 0, vmax = 100, cmap = 'bone');\n",
    "ax2.matshow(img_b, vmin = 0, vmax = 100, cmap = 'bone');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convince with statistics not subjective descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Intensity gradients\n",
    "Are the intensities constant in the image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(-1,1, 10)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6))\n",
    "ax1.matshow(xx, vmin = -1, vmax = 1, cmap = 'bone');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are even harder to describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting the right tool for your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Criteria\n",
    "There are many questions you have to ask about the data and analysis task you are facing. Here are some relevant questions that will guide you towards the choice of tool(s) you need to use when you analyses the data.\n",
    "- How many samples?\n",
    "- How complex is the sample?\n",
    "- Is human interpretation needed?\n",
    "- What is the end product?\n",
    "- Are there methods tools available?\n",
    "- Will there be more similar data sets?\n",
    "\n",
    "### The choice \n",
    "You have different options to choose the tool for your analysis. \n",
    "- Interactive tools\n",
    "- Scripting using existing toolboxes\n",
    "- Development of new algorithms\n",
    "Which one you actually select for your analysis workflow depends on many factors like the criteria listed in the previous section, but also on your experience, preferences in the group you are working with and so on.\n",
    "\n",
    "In general, there is __no golden recipe__! You have to be flexible from task to task.\n",
    "\n",
    "```{figure} figures/automation.png\n",
    "---\n",
    "---\n",
    "It sometimes feels decievingly simple to implement a script that automates your task... be careful!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column13\" >\n",
    "      \n",
    "**Criteria**\n",
    "      \n",
    "<font size=\"5em\">\n",
    "      \n",
    "- How many samples?\n",
    "- How complex is the sample?\n",
    "- Is human interpretation needed?\n",
    "- What is the end product?\n",
    "- Are there methods tools available?\n",
    "- Will there be more similar data sets?\n",
    "      \n",
    "  </font>  \n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "      \n",
    "**The choice** \n",
    "      \n",
    " <font size=\"5em\">\n",
    "     \n",
    "- Interactive tools\n",
    "- Scripting using existing toolboxes\n",
    "- Development of new algorithms\n",
    "      \n",
    "  </font>\n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "        <img src=\"figures/automation.png\" style=\"height:600px\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reproducibility vs. Repeatability\n",
    "\n",
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To reproduce an experiment means that someone else can repeat your entire workflow from sample preparation and experiment to the analysis to obtain the same results you got in your report. This should be possible completely independently for an experiment to be called reproducible.\n",
    "```{figure} figures/reproducibility.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A workflow describing the concept of reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='figures/reproducibility.svg' style='height:200px'>\n",
    "\n",
    "### Repeatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Repeatability is a more relaxed concept. It requires that someone else (or even yourself) can obtain the same results multiple time using the same data and analysis workflow. This means that there must very little room left for individual decisions that may have impact on the outcome.\n",
    "\n",
    "```{figure} figures/repeatability.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A workflow describing the concept of reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='figures/repeatability.svg' style='height:200px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reproducibility vs. Repeatability\n",
    "\n",
    "Science demands __repeatability__! and really wants __reproducability__\n",
    "- Experimental conditions can change rapidly and are difficult to make consistent\n",
    "- Animal and human studies are prohibitively time consuming and expensive to reproduce\n",
    "- Terabyte datasets cannot be easily passed around many different groups\n",
    "- Privacy concerns can also limit sharing and access to data\n",
    "\n",
    "----\n",
    "\n",
    "- _Science_ is already difficult enough\n",
    "- Image processing makes it even more complicated\n",
    "- Many image processing tasks are multistep, have many parameters, use a variety of tools, and consume a very long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we keep track of everything for ourselves and others?\n",
    "We can make the data analysis easy to repeat by an independent 3rd party\n",
    "- Document the analysis steps\n",
    "- Write clear and understandable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different views on image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Image analysis is a complex task and there are many ways to reach the quantitative results from the images. \n",
    "```{figure} figures/approaches.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Your background often decides how you approach an image analysis problem.\n",
    "```\n",
    "We can make two initial statements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"figures/approaches.png\" style=\"height:400px\"></center>\n",
    "\n",
    "- An image is a bucket of pixels.\n",
    "- How you choose to turn it into useful information is strongly dependent on your background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Experimentalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The experimentalist looks with a problem driven concept on the analysis task. It is often a top down approach aiming at solving the specific problem at hand. The solution is often reality driven and aims at finding models explaining the information presented in the images.\n",
    "\n",
    "Typical task the experimentalist tries to solve are very practical and specific like counting cells in the image or to measure the porosity of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "**Characteristics**\n",
    "-  Problem-driven \n",
    "- Top-down \n",
    "- _Reality_ Model-based \n",
    "\n",
    "**Examples**\n",
    "- cell counting\n",
    "- porosity\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <td><img src=\"figures/approaches.png\" style=\"height:500px;\"> </td>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Computer Vision Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The computer vision/signal processing scientist works to develop methods to solve a class of image processing problem. The approach is based on abstract features found in the image. The models are based on features and noise found in the images. The systematic appoach is even based on engineered image features to better test and evaluate the developed methods.\n",
    "\n",
    "The computer vision approach is typical looking to detect features like edges, structures, and also complicated features like faces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "     \n",
    "**Characteristics**\n",
    "- Method-driven\n",
    " - Feature-based\n",
    " - _Image_ Model-based\n",
    "- Engineer features for solving problems\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Edge detection\n",
    "- Face detection\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <td><img src=\"figures/approaches.png\" style=\"height:500px;\"> </td>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Analysis: Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, the deep learning approach is data driven and inspired by the way nature solves the image analysis problem. This approach rebuilds the way image processing is done from scratch, but at the same time it also based on concepts developed in computer vision. The deep learning approaches doesn't require a specific model describing the images it is meant to analyze, but rather make conclusions based on the previous images it has been exposed to. \n",
    "\n",
    "The deep learning appraoch is good handling rare events in the data and when it trained correctly it is also capable of generalizing to detect new features. This may sound like magic, but this is also a well founded and structured approach to analyzing images. Care most however be taken not to over fit or to genralize to much. The models are never better than the data they have been exposed to.\n",
    "\n",
    "Examples where deep learing is frequently used are to detect annomalies in the data or to label images based on their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "**Characteristics**\n",
    "- Results-driven\n",
    "- Biology ‘inspired’\n",
    "- Build both image processing and analysis from scratch\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Captioning images\n",
    "- Identifying unusual events\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "        <td><img src=\"figures/approaches.png\" style=\"height:500px;\"> </td>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Summary analysis approaches\n",
    "\n",
    "These three approaches have their own advantages and disadvantages, therefore it is good to know them all to be able to select the adquate method for the task you have to solve. It is not unusual that you will have to use a mix of the approaches. It is important to be open minded and think outside the box. In the end, what matters is that you can provide a reliable analysis of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The image formation process is the process to use some kind of excitation or impulse probe a sample. This requires the interaction of four parts.\n",
    " ```{figure} figures/image-formation.pdf\n",
    " ---\n",
    " ---\n",
    " The parts involved in the image formation process probing a sample.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/image-formation.svg\" />\n",
    "\n",
    "- __Impulses__ Light, X-Rays, Electrons, A sharp point, Magnetic field, Sound wave\n",
    "- __Characteristics__ Electron Shell Levels, Electron Density, Phonons energy levels, Electronic, Spins, Molecular mobility\n",
    "- __Response__ Absorption, Reflection, Phase Shift, Scattering, Emission\n",
    "- __Detection__ Your eye, Light sensitive film, CCD / CMOS, Scintillator, Transducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where do images come from?\n",
    "\n",
    "\n",
    "\n",
    "| Modality | Impulse | Characteristic | Response | Detection |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Light Microscopy| White Light | Electronic interactions | Absorption |Film, Camera |\n",
    "| Phase Contrast | Coherent light | Electron Density (Index of Refraction) | Phase Shift | Phase stepping, holography, Zernike |\n",
    "| Confocal Microscopy |Laser Light | Electronic Transition in Fluorescence Molecule | Absorption and reemission |Pinhole in focal plane, scanning detection |\n",
    "| X-Ray Radiography | X-Ray light | Photo effect and Compton scattering | Absorption and scattering | Scintillator, microscope, camera |\n",
    "| Neutron Radiography | Neutrons | Interaction with nucleus |Scattering and absorption| Scintillator, optics, camera |\n",
    "| Ultrasound |High frequency sound waves | Molecular mobility | Reflection and Scattering | Transducer |\n",
    "| MRI | Radio-frequency EM | Unmatched Hydrogen spins | Absorption and reemission | RF coils to detect |\n",
    "| Atomic Force Microscopy | Sharp Point | Surface Contact | Contact, Repulsion | Deflection of a tiny mirror|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Acquiring Images\n",
    "\n",
    "### Traditional / Direct imaging\n",
    "- Visible images produced or can be easily made visible\n",
    "- Optical imaging, microscopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\" here the measurement is supposed to be from a typical microscope which blurs, flips and otherwise distorts the image but the original representation is still visible\"",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bone_img  = imread('figures/tiny-bone.png').astype(np.float32)\n",
    "# simulate measured image\n",
    "conv_kern = np.pad(disk(2), 1, 'constant', constant_values = 0)\n",
    "meas_img  = convolve(bone_img[::-1], conv_kern)\n",
    "# run deconvolution\n",
    "dekern    = np.fft.ifft2(1/np.fft.fft2(conv_kern))\n",
    "rec_img   = convolve(meas_img, dekern)[::-1]\n",
    "# show result\n",
    "fig, (ax_orig, ax1, ax2) = plt.subplots(1,3, figsize = (15, 5))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone'); ax_orig.set_title('Original Object')\n",
    "ax1.imshow(np.real(meas_img), cmap = 'bone'); ax1.set_title('Measurement')\n",
    "ax2.imshow(np.real(rec_img), cmap = 'bone', vmin = 0, vmax = 255); ax2.set_title('Reconstructed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indirect / Computational imaging\n",
    "- Recorded information does not resemble object\n",
    "- Response must be transformed (usually computationally) to produce an image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.cap=\"here the measurement is supposed to be from a diffraction style experiment where the data is measured in reciprocal space (fourier) and can be reconstructed to the original shape\"",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bone_img = imread('figures/tiny-bone.png').astype(np.float32)\n",
    "# simulate measured image\n",
    "meas_img = np.log10(np.abs(np.fft.fftshift(np.fft.fft2(bone_img))))\n",
    "print(meas_img.min(), meas_img.max(), meas_img.mean())\n",
    "fig, (ax1, ax_orig) = plt.subplots(1,2, \n",
    "                               figsize = (12, 6))\n",
    "ax_orig.imshow(bone_img, cmap = 'bone')\n",
    "ax_orig.set_title('Original Object')\n",
    "\n",
    "ax1.imshow(meas_img, cmap = 'hot')\n",
    "ax1.set_title('Measurement');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traditional Imaging\n",
    "\n",
    "\n",
    "<img src=\"figures/traditional-imaging.png\" style=\"height:500px\"/>\n",
    "\n",
    "\n",
    "<small>\n",
    "Copyright 2003-2013 J. Konrad in EC520 lecture, reused with permission\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional Imaging: Model\n",
    "\n",
    "\n",
    "![Traditional Imaging Model](figures/traditional-image-flow.png)\n",
    "\n",
    "$$\n",
    "\\left[\\left([b(x,y)*s_{ab}(x,y)]\\otimes h_{fs}(x,y)\\right)*h_{op}(x,y)\\right]*h_{det}(x,y)+d_{dark}(x,y)\n",
    "$$\n",
    "\n",
    "$s_{ab}$ is the only information you are really interested in, so it is important to remove or correct for the other components\n",
    "\n",
    "For color (non-monochromatic) images the problem becomes even more complicated\n",
    "$$\n",
    "\\int_{0}^{\\infty} {\\left[\\left([b(x,y,\\lambda)*s_{ab}(x,y,\\lambda)]\\otimes h_{fs}(x,y,\\lambda)\\right)*h_{op}(x,y,\\lambda)\\right]*h_{det}(x,y,\\lambda)}\\mathrm{d}\\lambda+d_{dark}(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Indirect Imaging (Computational Imaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "With indirect imaging you make acquisitions in a form that don't represent the information you want to have. It is needed to perform a numeric transformation to obtain images in observation space.\n",
    "\n",
    "Some examples are:\n",
    "- Tomography through projections\n",
    "- Microlenses [Light-field photography](https://en.wikipedia.org/wiki/Light-field_camera)\n",
    "- Diffraction patterns\n",
    "- Hyperspectral imaging with Raman, IR, CARS\n",
    "- Surface Topography with cantilevers (AFM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table>\n",
    "<tr><td>\n",
    "    \n",
    "- Tomography through projections\n",
    "- Microlenses [Light-field photography](https://en.wikipedia.org/wiki/Light-field_camera)\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- Diffraction patterns\n",
    "- Hyperspectral imaging with Raman, IR, CARS\n",
    "- Surface Topography with cantilevers (AFM)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><video controls loop src=\"movies/lightfield.mp4\" height=\"300px\" type=\"video/mp4\"></video></td>\n",
    "<td><img src=\"figures/surface-plot.png\" style=\"height:300px\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images \n",
    "\n",
    "## An introduction to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### What is an image?\n",
    "\n",
    "A very abstract definition: \n",
    "- __A pairing between spatial information (position)__\n",
    "- __and some other kind of information (value).__\n",
    "\n",
    "In most cases this is a 2- or 3-dimensional position with \n",
    "- x,y,z coordinates on a grid\n",
    "- and a numeric value (intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image sampling\n",
    "| The world is | The computer needs|\n",
    "|:---:|:---:|\n",
    "| Continuous    | Discrete levels |\n",
    "| No boundaries | Limited extent | \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/grid.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "The real world is sampled into discrete images with limited extent.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"figures/grid.svg\" style=\"height:400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does spatial sampling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "img=np.load('../../common/data/wood.npy');\n",
    "plt.figure(figsize=[15,7])\n",
    "plt.subplot(2,3,1); plt.imshow(img); plt.title('Original')\n",
    "downsize =  2; plt.subplot(2,3,2); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize), anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "downsize = 32; plt.subplot(2,3,3); plt.imshow(resize(img,(img.shape[0] // downsize, img.shape[1] // downsize),anti_aliasing=False)); plt.title('Downsize {0}x{0}'.format(downsize))\n",
    "levels   = 16; plt.subplot(2,3,5); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));\n",
    "levels   = 4 ; plt.subplot(2,3,6); plt.imshow(np.floor(img*levels)); plt.title('{0} Levels'.format(levels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pixel size and resolution \n",
    "\n",
    "It is important to distinguish between pixel size and resolution\n",
    "\n",
    "### Pixel size\n",
    "The pixel size is\n",
    "- The sample pitch between two adjacent pixels\n",
    "- The smallest area represented in the image\n",
    "\n",
    "### Resolution \n",
    "The resolution is related to the optical system\n",
    "- It is the effect of the optical transfer function of the acquisition system.\n",
    "- Should have a greater value than the pixel size.\n",
    "- Defines the smallest pixel size when you set up your acquistion conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demonstrating different pixel sizes\n",
    "What happens when we represent the same image with less pixels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we downsample the image first by a factor two. This change is barely visible when we show the image, but the number of pixels have reduced by a factor four. In the second example the image is downscaled by a factor 32 and you can clearly observe how pixelated the image is. A this level of downscaling, you can only see very coarse features in the sample.\n",
    "\n",
    "Down scaling is sometimes used as a method to speed up the frame rate as it radiacally reduces the number of bytes to be transfered from the detector and also the amount of data to write on disk. You should however be careful not to down scale by a too great factor as you will loose spatial information when doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "img=np.load('../../common/data/wood.npy');\n",
    "fig,ax = plt.subplots(1,3, figsize=[15,5])\n",
    "ax[0].imshow(img,cmap='gray'); plt.title('Original')\n",
    "\n",
    "downsize =  2; \n",
    "resized  = resize(img,(img.shape[0] // downsize, img.shape[1] // downsize))\n",
    "ax[1].imshow(resized, interpolation='None',cmap='gray'); ax[1].set_title('Downsize {0}x{0}'.format(downsize))\n",
    "\n",
    "downsize = 32; \n",
    "resized  = resize(img,(img.shape[0] // downsize, img.shape[1] // downsize))\n",
    "ax[2].imshow(resized,interpolation='None',cmap='gray'); ax[2].set_title('Downsize {0}x{0}'.format(downsize));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Edges at different resolutions and pixel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finding the correct pixel size is related to the resolution of the imaging system. You can sample low resolved scenes with many pixels but then the edges will appear blurred and will essentially waste a lot data on little added value.\n",
    "\n",
    "The example below shows what an ideal edge would look like and what it mostly looks like when we acquire our images. As you can see, the \"real\" edge is represented by a smooth transition spread over several pixels.\n",
    "\n",
    "```{figure} figures/edge.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Examples of edges sampled with different pixel sizes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/edges.svg\" style=\"height:600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image intensity\n",
    "What happens when we reduce the number of gray-levels in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The image intensity is determined by the response function of the imaging system. In the case of neutron imaging we are talking about the transmission of the neutron beam through the sample. The transmission follows Beer-Lambert's law\n",
    "$I(x,y)=I(x,y) e^{-\\int_L \\mu(x) dx}$, this is only a simplified version. More complicated versions including the neutron energy are presented in other parts of this course.\n",
    "\n",
    "The information captured by the detector is stored in digital form with different gray level dynamics. We are often talking about 8 or 16 bit integer when we store images. This means that each pixel can represent the measured intensity with either 256 or 65565 gray levels respectively. In the example below we demonstrate what happens when only very few gray levels are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "img=np.load('../../common/data/wood.npy');\n",
    "fig,ax = plt.subplots(1,3,figsize=[15,7])\n",
    "ax[0].imshow(img, cmap='gray'); plt.title('Original')\n",
    "\n",
    "levels   = 16; \n",
    "lvl = np.floor(img*levels)\n",
    "ax[1].imshow(lvl, cmap='gray'); ax[1].set_title('{0} Levels'.format(levels));\n",
    "\n",
    "levels   = 4 ; \n",
    "lvl = np.floor(img*levels)\n",
    "ax[2].imshow(lvl, cmap='gray'); ax[2].set_title('{0} Levels'.format(levels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is important to use as many gray levels at possible when you expose your images. The image turns patchy when you use too few levels which you can see in the example above. The patchiness reduces the precision of your evaluation, there is less margin to make estimations and decisions. The number of gray levels depend on many factors like:\n",
    "\n",
    "- Exposure time\n",
    "- Source intensity\n",
    "- Conversion efficiency\n",
    "- Conversion rate of the detector\n",
    "- Pixel size\n",
    "\n",
    "So, it is your task to optimize your acquisition to provide well illuminated images by changing these parameters. Some are easier than others to change and contstraint are given by the type of investigation you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many bits are needed?\n",
    "\n",
    "The number of bits you need depends on:\n",
    "- Contrast difference\n",
    "- Separate many different sample features\n",
    "- Sensitivity to rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The table below gives you an idea how many bits you need to represent your image information. In the extreme you would only need a single bit per pixel (8 pixels per byte) to represent a bi-level image from a segmentation. The other extreme would be to use double precision floating point that requires 64 bits (8 bytes) per pixels. Double precision is rarely needed and single precision is mostly sufficient which saves you memory. Saving memory is in particular important when you work with 3D images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">\n",
    "\n",
    "| Few bits | Many bits | Floating point |\n",
    "|:----------|:-----------|:----------------|\n",
    "|High contrast<br/>Clean images<br/>Segmented data|Low contrast<br/>Noisy images<br/>Gradual changes|High intensity dynamics<br/>Quantification to physical properties<br/>In algorithms|\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The histogram\n",
    "\n",
    "The histogram is a statistical tool to show frequency of each graylevel in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is essentially a plot where the you count how many times each gray level appears in the image. For a 16-bit image this would result 65565 points. This is far too detailed therefore it is common to use bins of several gray level to reduce the level of detail in the histogram and also improve the readability. In the example below we use 100 histogram bins which look quite reasonable for this image. Chosing the number of bins depends on the image size too. Your histogram doesn't look very useful if you have too many bins compared to the available number of pixels. You can use the piece of code below to explore what happens when you change the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "# Compute and show a histogram\n",
    "ax[0].hist(img.ravel(),bins=200)\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Image value'), ax[0].set_ylabel('Number of pixels')\n",
    "ax[1].imshow(img,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It can be used for the analysis of the image as it gives you an idea which values are related to different features in the image. The histogram tells you the area covered by a give pixel value and a later section we will see how the histogram can be used to segment the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of the histogram\n",
    "Let's look att different regions in the image and their representation in the histogram.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_regions.svg\" style=\"height:400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/histogram_regions.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Regions in the image connected to their position in the histogram.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Histogram examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "a=plt.imread('figures/testpattern_noisy.jpg')\n",
    "b=plt.imread('figures/neutron_camera.png')\n",
    "c0=plt.imread('figures/root_slices.png')\n",
    "c1=plt.imread('figures/root_histogram.png')\n",
    "\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,8))\n",
    "ax=ax.ravel()\n",
    "\n",
    "ax[0].imshow(a,cmap='gray')\n",
    "ax[0].set_xticks([]);ax[0].set_yticks([])\n",
    "ax[0].set_title('Noisy bi-level image')\n",
    "ax[3].hist(a.ravel(),bins=100)\n",
    "ax[1].imshow(b,cmap='gray')\n",
    "ax[1].set_title('Neutron image')\n",
    "ax[1].set_xticks([]);ax[1].set_yticks([])\n",
    "ax[4].hist(b.ravel(),bins=100)\n",
    "ax[2].imshow(c0)\n",
    "ax[2].set_title('Bivariate data')\n",
    "ax[2].axis('off')\n",
    "ax[5].imshow(c1)\n",
    "ax[5].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing gray levels\n",
    "\n",
    "The human eye is not able to resolve many intensity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we demonstrate how easy or hard it can be to perceive contrast differences in an image with different number of gray levels. How well you can see the differences depends on one hand on how well your eye can resolve the contrast difference, but there are also technical issues related to how well you can see the changes. E.g. how well your screen can display the changes and even the ambient light in the room you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xlin = np.linspace(0,255, 256)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize = (15, 5))\n",
    "n=32; ax[0].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[0].set_title('{0} levels'.format(256//n))\n",
    "n=8;  ax[1].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[1].set_title('{0} levels'.format(256//n))\n",
    "n=1;  ax[2].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[2].set_title('{0} levels'.format(256//n));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brightness\n",
    "With image brightness, you can focus on narrow gray level intervals to better visually resolve local details.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_brightness.svg\" style=\"height:500px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/histogram_brightness.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Narrow intensity intervals to highlight low (left) and high (right) graylevel regions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Contrast\n",
    "\n",
    "Contrast controls the width of the intensity interval to use.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_contrast.svg\" style=\"height:500px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Contrast control is often used to define which gray levels to include when you save image to file. The image is usually represented in floating point data format after some calculations and you have to limit the interval to resolve the relevant information with many gray levels and reject outliers when you convert to 8- or 16-bit integers. \n",
    "\n",
    "The example shows a narrow interval that mostly is useful to highlight features with small difference in contrast. The example with wider interval would is set to reject the background while most of the sample is visible. This setting may be useful for presentationations and publication where you want to boost the visibility, but is not recommended if you want to use the image in further calculations. In the latter case it is important to keep as many gray levels as possible, i.e. also include the noise flucuations in the background.\n",
    "\n",
    "```{figure} figures/histogram_contrast.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Different image contrasts. Narrow interval to the left and wide interval to the right.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D Intensity Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "basic_image = np.random.choice(range(100), size = (5,5))\n",
    "\n",
    "xx, yy   = np.meshgrid(range(basic_image.shape[1]), range(basic_image.shape[0]))\n",
    "image_df = pd.DataFrame(dict(x = xx.ravel(),\n",
    "                 y = yy.ravel(),\n",
    "                 Intensity = basic_image.ravel()))\n",
    "image_df[['x', 'y', 'Intensity']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viewing the data as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "plot_image = ax1.matshow(basic_image, cmap = 'gray')\n",
    "plt.colorbar(plot_image)\n",
    "\n",
    "for _, c_row in image_df.iterrows():\n",
    "    ax1.text(c_row['x'], c_row['y'], s = '%02d' % c_row['Intensity'], fontdict = dict(color = 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's colorize our image\n",
    "\n",
    "The next step is to apply a color map (also called lookup table, LUT) to the image \n",
    "- so it is a bit more exciting \n",
    "- some features are easier to detect [Rogowitz et al. 1996](https://doi.org/10.1063/1.4822401)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using pseudo colormaps\n",
    "\n",
    "The image intensity is mostly only represented by a scalar by a gray level. Which is makes it hard to see subtle changes in intensity. Colormaps can help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "ax=ax.ravel()\n",
    "\n",
    "a0=ax[0].matshow(basic_image, cmap = 'Blues')\n",
    "fig.colorbar(a0,ax=ax[0],shrink=0.75)\n",
    "a1=ax[1].matshow(basic_image, cmap = 'jet')\n",
    "fig.colorbar(a1,ax=ax[1],shrink=0.75)\n",
    "a2=ax[2].matshow(basic_image, cmap = 'viridis')\n",
    "fig.colorbar(a2,ax=ax[2],shrink=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pseudo colors on real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(15,5))\n",
    "ax[0].imshow(img,cmap='Blues');   ax[0].set_title('Blues')\n",
    "ax[1].imshow(img,cmap='jet');     ax[1].set_title('Jet')\n",
    "ax[2].imshow(img,cmap='viridis'); ax[2].set_title('Viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Lookup Tables\n",
    "\n",
    "Formally a color map is lookup table or a function which\n",
    "$$ f(\\textrm{Intensity}) \\rightarrow \\textrm{Color} $$\n",
    "\n",
    "#### Matplotlib's color maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<center>Never use rainbox maps like jet, see <a href=\"https://agilescientific.com/blog/2017/12/14/no-more-rainbows\">No more rainbows!</a></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "#from colorspacious import cspace_converter\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict() \n",
    "cmaps['Perceptually Uniform Sequential'] = [\n",
    "            'viridis', 'plasma', 'inferno', 'magma', 'cividis']\n",
    "\n",
    "cmaps['Sequential'] = [\n",
    "            'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "cmaps['Sequential (2)'] = [\n",
    "            'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "            'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "            'hot', 'afmhot', 'gist_heat', 'copper']\n",
    "\n",
    "cmaps['Diverging'] = [\n",
    "            'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "            'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']\n",
    "\n",
    "cmaps['Cyclic'] = ['twilight', 'twilight_shifted', 'hsv']\n",
    "\n",
    "cmaps['Qualitative'] = ['Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                        'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                        'tab10', 'tab20', 'tab20b', 'tab20c']\n",
    "\n",
    "cmaps['Miscellaneous'] = [\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar']\n",
    "\n",
    "\n",
    "nrows = max(len(cmap_list) for cmap_category, cmap_list in cmaps.items())\n",
    "\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list, nrows):\n",
    "    fig, axes = plt.subplots(nrows=nrows)\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.2, right=0.99)\n",
    "    axes[0].set_title(cmap_category + ' colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axes, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        pos = list(ax.get_position().bounds)\n",
    "        x_text = pos[0] - 0.01\n",
    "        y_text = pos[1] + pos[3]/2.\n",
    "        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "for cmap_category, cmap_list in cmaps.items():\n",
    "    plot_color_gradients(cmap_category, cmap_list, nrows)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How are the colors combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.linspace(0, 1, 100)\n",
    "colors = ['Red','Green','Blue']\n",
    "plt.figure(figsize=[15,4])\n",
    "for i in np.arange(0,3) :\n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.hot(xlin)[:,i],\n",
    "            c = plt.cm.hot(xlin),label=\"hot\")\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.Blues(xlin)[:,i], \n",
    "            c = plt.cm.Blues(xlin),label=\"blues\")\n",
    "\n",
    "    plt.scatter(xlin, \n",
    "            plt.cm.jet(xlin)[:,i], \n",
    "            c = plt.cm.jet(xlin),label='jet')\n",
    "\n",
    "    plt.xlabel('Intensity');\n",
    "    plt.ylabel('{0} Component'.format(colors[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applied LUTs\n",
    "These transformations can also be non-linear as is the case of the graph below where the mapping between the intensity and the color is a $\\log$ relationship meaning the the difference between the lower values is much clearer than the higher ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xlin = np.logspace(-2, 5, 500)\n",
    "log_xlin = np.log10(xlin)\n",
    "norm_xlin = (log_xlin-log_xlin.min())/(log_xlin.max()-log_xlin.min())\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(norm_xlin)[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "\n",
    "ax1.scatter(xlin, plt.cm.hot(xlin/xlin.max())[:,0], \n",
    "            c = plt.cm.hot(norm_xlin))\n",
    "ax1.set_xscale('log');ax1.set_xlabel('Intensity');ax1.set_ylabel('Red Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LUTs on real images\n",
    "\n",
    "On a real image the difference is even clearer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=5",
    "autoscroll": false,
    "hide_input": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (12, 4))\n",
    "in_img = imread('figures/bone-section.png')[:,:,0].astype(np.float32)\n",
    "ax1.imshow(in_img, cmap = 'gray');\n",
    "ax1.set_title('grayscale LUT');\n",
    "\n",
    "ax2.imshow(in_img, cmap = 'hot');\n",
    "ax2.set_title('hot LUT');\n",
    "\n",
    "ax3.imshow(np.log2(in_img+1), cmap = 'gray');\n",
    "ax3.set_title('grayscale-log LUT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Colormaps are only used for the visulization making it possible to better visualize and highlight features in the image. They can however also be misleading if you chose the wrong colormap. The interpretation of the image is in particular hard when you start manipulating the colormap. In this way it is even posible to \"invent\" features in the image that are not real. A typical example is that you thanks to the colormap could see a denser skin like structure near the sample boundary. It the reality this \"skin\" is only the smooth edge which is caused by low resolution of the imaging system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pixelwise operations\n",
    "\n",
    "Pixelwise operations apply scalar operations to each pixel.\n",
    "- Arithmetics +, -, *, /\n",
    "- Functions e.g. sin(x), exp(x), ln(x)\n",
    "\n",
    "Statistic functions\n",
    "- mean, standard deviation\n",
    "- min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demonstrating arithmetic functions (flat field normalization)\n",
    "\n",
    "$$normed = \\frac{img-dc}{ob-dc}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/wood_0000.tif')\n",
    "ob  = plt.imread('data/ob_0000.tif')\n",
    "dc  = plt.imread('data/dc_0000.tif')\n",
    "\n",
    "normed = (img-dc)/(ob-dc)\n",
    "\n",
    "# Visualization\n",
    "fig,ax = plt.subplots(1,4,figsize=(15,4))\n",
    "ax[0].imshow(img,    cmap='gray'); ax[0].set_title('Sample image')\n",
    "ax[1].imshow(ob,     cmap='gray'); ax[1].set_title('Open beam image')\n",
    "ax[2].imshow(dc,     cmap='gray'); ax[2].set_title('Dark current image')\n",
    "ax[3].imshow(normed, cmap='gray'); ax[3].set_title('Normalized image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3D Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For a 3D image, the position or spatial component has a 3rd dimension (z if it is a spatial, or t if it is a movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<table><tr><td>Volume</td><td>Time series</td></tr>\n",
    "    <tr>\n",
    "    <td><img src=\"figures/cube_10x10x10.svg\"></td>\n",
    "    <td><img src=\"figures/timeseries_visualization.svg\">\n",
    "</td><tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/cube_10x10x10.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Three-dimensional data can be a volume in space.\n",
    "```\n",
    "\n",
    "```{figure} figures/timeseries_visualization.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A movie can also be seen as a three-dimensional image.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A 3D image as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vol_image = np.arange(27).reshape((3,3,3))\n",
    "print(vol_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Showing 2D slices from volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=10",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import montage as montage2d\n",
    "print(montage2d(vol_image, fill = 0))\n",
    "plt.matshow(montage2d(vol_image, fill = 0), cmap = 'jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4D Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/4D-images.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Four-dimensional data are series of volumes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/4D-images.svg\" style=\"height:500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Values per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the images thus far, we have had one value per position, but there is no reason there cannot be multiple values. In fact this is what color images are (red, green, and blue) values and even 4 channels with transparency (alpha) as a different. For clarity we call the __dimensionality__ of the image the number of dimensions in the spatial position, and the __depth__ the number in the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "results='asis'",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "base_df = pd.DataFrame([dict(x = x, y = y) for x,y in product(range(5), range(5))])\n",
    "base_df['Intensity'] = np.random.uniform(0, 1, 25)\n",
    "base_df['Transparency'] = np.random.uniform(0, 1, 25)\n",
    "base_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This can then be rearranged from a table form into an array form and displayed as a series of slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Display multi-valued pixels separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The most straight forward way to display multiple pixel values is to display each value separately. This method is, however, mostly not very suitable as the values often are related in some sense. Therefore it is recommended to combine the values in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=8, fig.width = 6",
    "autoscroll": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Intensity']), s = 1000)\n",
    "ax1.set_title('Intensity')\n",
    "ax2.scatter(base_df['x'], base_df['y'], c = plt.cm.gray(base_df['Transparency']), s = 1000)\n",
    "ax2.set_title('Transparency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we combined two values use one value to control the colormap and the other to control the size of the dots. How you combine the data is related to the to of data you want to combine. If the values are components of a vector it makes more sense to show arrows of different length and direction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "ax1.scatter(base_df['x'], base_df['y'], c = plt.cm.jet(base_df['Intensity']), s = 1000*base_df['Transparency'])\n",
    "ax1.set_title('Intensity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperspectral Imaging\n",
    "\n",
    "\n",
    "At each point in the image (black dot), instead of having just a single value, there is an entire spectrum. A selected group of these (red dots) are shown to illustrate the variations inside the sample. While certainly much more complicated, this still constitutes and image and requires the same sort of techniques to process correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "load_hypermap",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import os\n",
    "raw_img = imread('../../common/data/raw.jpg')\n",
    "im_pos = pd.read_csv('../../common/data/impos.csv', header = None)\n",
    "im_pos.columns = ['x', 'y']\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 8));\n",
    "ax1.imshow(raw_img);\n",
    "ax1.scatter(im_pos['x'], im_pos['y'], s = 1, c = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking at the pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Each pixel of this data set is represented by a full spectrum. This means that we have both wave numbers and intensity values. In many cases, the wavenumbers are the same for all pixels, which makes it possible to reduce the redundancy of the wave number vector using a single 1D array to represent this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../../common/data/full_img.csv').query('wavenum<1200')\n",
    "print(full_df.shape[0], 'rows')\n",
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "full_df['g_x'] = pd.cut(full_df['x'], 5)\n",
    "full_df['g_y'] = pd.cut(full_df['y'], 5)\n",
    "fig, m_axs = plt.subplots(9, 3, figsize = (15, 12)); m_axs=m_axs.ravel()\n",
    "for ((g_x, g_y), c_rows), c_ax in zip(full_df.sort_values(['x','y']).groupby(['g_x', 'g_y']),m_axs):\n",
    "    c_ax.plot(c_rows['wavenum'], c_rows['val'], 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workflows for image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing has changed: Parallel\n",
    "\n",
    "### Moores Law\n",
    "$$ \\textrm{Transistors} \\propto 2^{T/(\\textrm{18 months})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Borrowed from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d\n",
    "moores_txt=[\"Id Name  Year  Count(1000s)  Clock(MHz)\\n\",\n",
    "        \"0            MOS65XX  1975           3.51           14\\n\",\n",
    "        \"1          Intel8086  1978          29.00           10\\n\",\n",
    "        \"2          MIPSR3000  1988         120.00           33\\n\",\n",
    "        \"3           AMDAm486  1993        1200.00           40\\n\",\n",
    "        \"4        NexGenNx586  1994        3500.00          111\\n\",\n",
    "        \"5          AMDAthlon  1999       37000.00         1400\\n\",\n",
    "        \"6   IntelPentiumIII  1999       44000.00         1400\\n\",\n",
    "        \"7         PowerPC970  2002       58000.00         2500\\n\",\n",
    "        \"8       AMDAthlon64  2003      243000.00         2800\\n\",\n",
    "        \"9    IntelCore2Duo  2006      410000.00         3330\\n\",\n",
    "        \"10         AMDPhenom  2007      450000.00         2600\\n\",\n",
    "        \"11      IntelCorei7  2008     1170000.00         3460\\n\",\n",
    "        \"12      IntelCorei5  2009      995000.00         3600\"]\n",
    "\n",
    "sio_table = StringIO(''.join(moores_txt)); moore_df = pd.read_table(sio_table, sep = '\\s+', index_col = 0);\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8, 4)); ax1.semilogy(moore_df['Year'], moore_df['Count(1000s)'], 'b.-', label = '1000s of transitiors'); ax1.semilogy(moore_df['Year'], moore_df['Clock(MHz)'], 'r.-', label = 'Clockspeed (MHz)') ;ax1.legend(loc = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<small>_Based on data from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d_</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why doesn't the clock rate follow Moore's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are now many more transistors inside a single computer but the CPU clock rate hasn't increased. It is actually stagnating since the early 2000s. The reason for this is that the number of processing cores has started to increase. Each core is a CPU is an autonomous CPU which makes it possible to perform many individual tasks in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Multiple Core\n",
    "- Multiple CPUs\n",
    "- New modalities\n",
    "  - GPUs provide many cores which operate at slow speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The conclusion is that we need to make the processing on multiple Cores/CPUs possible. This can be done by chosing multithreaded modules for the processing. In the extreme case we need to implement new multi-process code for the analysis. How this is done depends in the target hardware. You will use different approaches for a CPU than you use in a cluster or in cloud computing. In general, we can conclude:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Parallel Code is important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing has changed: Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Cloud computing is a type of service that has evolved from the big data era. Large companies have built up their analysis resources to match the need at peak demand. These resources were often underused which resulted in a new business model. Providing computing resources to external customers. In this way the customers only pay for the computing time they need and let someone else handle investments and system maintenance.\n",
    "\n",
    "```{figure} figures/cloud-services.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Different cloud services where it is possible to buy CPU time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Computer, servers, workstations are _wildly underused_ (majority are <50%)\n",
    "- Buying a big computer that sits _idle most of the time_ is a waste of money\n",
    "    - http://www-inst.eecs.berkeley.edu/~cs61c/sp14/\n",
    "    - “The Case for Energy-Proportional Computing,” Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007\n",
    "- Traditionally the most important performance criteria was time, how fast can it be done\n",
    "- With Platform as a service servers can be _rented instead of bought_\n",
    "- Speed is still important but using cloud computing $ / Sample is the real metric\n",
    "- In Switzerland a PhD student is 400x as expensive per hour as an Amazon EC2 Machine\n",
    "- Many competitors keep prices low and offer flexibility\n",
    "\n",
    "\n",
    "<img src=\"figures/cloud-services.png\" style=\"height:400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workflow analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It sound like a great idea to perform tasks in parallel, but how do we bring the compoute to do so. The following example from real life is used to demonstrate how a greater task can be broken down into smaller piecese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Simple Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to cook a simple soup. There is already a recipe telling us what to do to produce a soup. It is easy to follow the list, anyone with the right steps can execute and repeat (if not reproduce) the soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Buy {carrots, peas, tomatoes} at market\n",
    "1. _then_ Buy meat at butcher\n",
    "1. _then_ Chop carrots into pieces\n",
    "1. _then_ Chop potatos into pieces\n",
    "1. _then_ Heat water\n",
    "1. _then_ Wait until boiling then add chopped vegetables\n",
    "1. _then_ Wait 5 minutes and add meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All steps are done in a sequence to produce this soup. This would correspond to performing the task on a single CPU. Now, how can we change this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More complicated soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The following recipe is harder to follow and you need to carefully keep track of what is being performed. If you look at it in detail, you will see that some of the steps can be performed independently of the others. This is our opportunity to dispatch parallel tasks that would be running on different CPUs. In the kitchen, you could delegate tasks to other people and thus shorten the time until the soup is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Steps 1-4__\n",
    "\n",
    "4. _then_ Mix carrots with potatos $\\rightarrow  mix_1$\n",
    "\n",
    "4. _then_ add egg to $mix_1$ and fry for 20 minutes\n",
    "\n",
    "4. _then_ Tenderize meat for 20 minutes\n",
    "\n",
    "4. _then_ add tomatoes to meat and cook for 10 minutes $\\rightarrow mix_2$\n",
    "\n",
    "5. _then_ Wait until boiling then add $mix_1$\n",
    "\n",
    "6. _then_ Wait 5 minutes and add $mix_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using flow charts / workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The processing task can be visualize using flow charts to better get an overview of the steps to be done. A sequence has a trivial shape, but it hard to get the big picture as soon as you start creating more complicated schemes ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Workflow of the Simple Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we use the graph plotting module graphviz to draw the flow chart of the soup example. Graphviz is only a plotting tool with no direct processing capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n) \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "for (c_n, d_n) in zip(nodes, nodes[1:]):\n",
    "    graph.add_edge(pydot.Edge(c_n, d_n))\n",
    "\n",
    "plt = Image(graph.create_png()); display(plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Workflows - the complicated soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Clearly a linear set of instructions is ill-suited for even a fairly easy soup, it is then even more difficult when there are dozens of steps and different pathsways.\n",
    "\n",
    "Furthermore a clean workflow allows you to better parallelize the task since it is clear which tasks can be performed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "fig.height=9",
    "autoscroll": false,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import pydot\n",
    "graph = pydot.Dot(graph_type='digraph', rankdir=\"LR\")\n",
    "node_names = [\"Buy\\nvegetables\",\"Buy meat\",\"Chop\\nvegetables\",\"Heat water\", \"Add Vegetables\",\n",
    "              \"Wait for\\nboiling\",\"Wait 5\\nadd meat\"]\n",
    "nodes = [pydot.Node(name = '%04d' % i, label = c_n, style = 'filled') \n",
    "         for i, c_n in enumerate(node_names)]\n",
    "for c_n in nodes:\n",
    "    graph.add_node(c_n)\n",
    "    \n",
    "def e(i,j, col = None):\n",
    "    if col is not None:\n",
    "        for c in [i,j]:\n",
    "            if nodes[c].get_fillcolor() is None: \n",
    "                nodes[c].set_fillcolor(col)\n",
    "    graph.add_edge(pydot.Edge(nodes[i], nodes[j]))\n",
    "\n",
    "e(0, 2, 'gold'); e(2, 4); e(3, -2, 'springgreen'); e(-2, 4, 'orange'); e(4, -1) ; e(1, -1, 'dodgerblue')\n",
    "plt = Image(graph.create_png()); display(plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Directed Acyclical Graphs (DAG)\n",
    "- We can represent almost any computation without loops as DAG. \n",
    "- This allows us to break down a computation into pieces which can be carried out independently. \n",
    "\n",
    "There are a number of tools which let us handle this issue.\n",
    "\n",
    "- PyData Dask - https://dask.pydata.org/en/latest/\n",
    "- Apache Spark - https://spark.apache.org/\n",
    "- Spotify Luigi - https://github.com/spotify/luigi\n",
    "- Airflow - https://airflow.apache.org/\n",
    "- KNIME - https://www.knime.com/\n",
    "- Google Tensorflow - https://www.tensorflow.org/\n",
    "- Pytorch / Torch - http://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Concrete example - Creating a DAG\n",
    "What is a DAG good for?\n",
    "\n",
    "#### Create two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let create a DAG to demonstrate how it works. This first piece creates two small images (5x5 pixels) each containing either '0's or '1's. We still haven't started any calculations using this DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((5,5), chunks = (5,5))\n",
    "image_2 = da.ones((5,5), chunks = (5,5))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Add two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Adding two variables in a DAG looks like the code to add any variables. Now if we look at the graph created by this operation, we that there is a directed flow of the data in this graph.\n",
    "\n",
    "$$Image_3 = Image_1 + Image_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_3 = image_1 + image_2\n",
    "dot_graph(image_3.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### A more complicated operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now lets make a more complicated calculation like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$Image_4=(image_1-10)+(image_2*50)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can here see that there are two clear branches in the flow chart. This indicates that here is an opportunity to perform these tasks in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Let's go big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now let's see where this can be really useful. Dask allows you to split the data into smaller pieces (chunks). In the example below we have created a 1024x1024 pixels image and also told dask that this image should be divided into chunks of 512x512 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Creating a large image with chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.dot import dot_graph\n",
    "image_1 = da.zeros((1024, 1024), chunks = (512, 512))\n",
    "image_2 = da.ones((1024 ,1024), chunks = (512, 512))\n",
    "dot_graph(image_1.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You see now that there are four graph created. The boxes on the top indicates the location of the chunk in the large image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Computing something with a larger image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we now apply the \"complicated\" calculation \n",
    "\n",
    "$$Image_4=(image_1-10)+(image_2*50)$$\n",
    "\n",
    "on the large image with chunks, we see that the code hasn't changed at all. When we look at the graph generated with this data, we see that there are four graphs repeating the same operations. One graph for each chunk we defined in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_4 = (image_1-10) + (image_2*50)\n",
    "dot_graph(image_4.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Matrix multiplication in a DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A matrix multiplication is a more complicated operation than the previous exmaples. This operation needs information from all the other chunks to calulate the current chunk. This can also be seen in the wiring of the graph below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_5 = da.matmul(image_1, image_2)\n",
    "dot_graph(image_5.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### An even more complicated computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Combining different operations into a final calculation like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$Image_6=(Image_1\\cdot{}Image_2+Image_1)\\circ{}Image_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "image_6 = (da.matmul(image_1, image_2)+image_1)*image_2\n",
    "dot_graph(image_6.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We see here that the convolution part connects all branches of the graph which mean all processing must be synchronized here. Afterwards there are no connections and each sub graph can work independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Convolution using a DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Convolution makes life harder for a DAG. There is a need for an overlap between the chunks to avoid boundary effects. These boundary effects are something we will look into in next week's lecture about filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$Image_7=Image_6*Image_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import dask_ndfilters as da_ndfilt\n",
    "image_7 = da_ndfilt.convolve(image_6, image_1)\n",
    "dot_graph(image_7.dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Deep Learning\n",
    "We won't talk too much about deep learning now, but it certainly shows why DAGs are so important. \n",
    "\n",
    "![DAG-NN](figures/DAG-shallow-NN.png)\n",
    "\n",
    "The steps above are simple toys compared to what tools are already in use for machine learning\n",
    "\n",
    "https://keras.io/api/utils/model_plotting_utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this lecture we saw that:\n",
    "- Images revieal information about different samples\n",
    "- Images are a signals that needs to be quantitatively analyzed\n",
    "- Science with images is a non-trivial task\n",
    "- Proper workflows are required for efficient analysis repeatable analysis.\n",
    "\n",
    "## Next weeks lecture\n",
    "\n",
    "- Data sets\n",
    "- Data augmentation\n",
    "- Ground truth"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "autolaunch": true,
   "footer": "February 24, 2022 - ETH 227-0966-00L: Quantitative Big Imaging - Introduction",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
