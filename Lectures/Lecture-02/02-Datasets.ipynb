{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ground Truth: Building and Augmenting Datasets\n",
    "__Quantitative Big Imaging__ ETHZ: 227-0966-00L\n",
    "\n",
    "<p style=\"font-size:1em;\">March 3, 2022</p>\n",
    "<br /><br />\n",
    "<p style=\"font-size:1.5em;padding-bottom: 0.25em;\">Anders Kaestner</p>  \n",
    "<p style=\"font-size:1em;\">Laboratory for Neutron Scattering and Imaging<br />Paul Scherrer Institut</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's lecture\n",
    "\n",
    "__Creating Datasets__\n",
    "- Famous Datasets\n",
    "- Types of Datasets\n",
    "- What makes a good dataet?\n",
    "- Building your own \n",
    " - \"scrape, mine, move, annotate, review, and preprocess\" - Kathy Scott\n",
    " - tools to use\n",
    " - simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Augmentation__\n",
    "- How can you artifically increase the size of your dataset?\n",
    "- What are the limits of these increases\n",
    "\n",
    "__Baselines__\n",
    "- What is a baseline?\n",
    "- Example: Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's load some modules for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib    as mpl\n",
    "import numpy         as np\n",
    "import skimage       as ski\n",
    "import skimage.io    as io\n",
    "from skimage.morphology import disk\n",
    "import scipy.ndimage as ndimage\n",
    "from keras.datasets  import mnist\n",
    "from skimage.util    import montage as montage2d\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [Revisiting **Unreasonable Effectiveness of Data** in Deep Learning Era](https://arxiv.org/abs/1707.02968)\n",
    "- [Data science ... without any data](https://towardsdatascience.com/data-science-without-any-data-6c1ae9509d92)\n",
    "- Building Datasets\n",
    "    - Python Machine Learning 2nd Edition by Sebastian Raschka, Packt Publishing Ltd. 2017\n",
    "     - Chapter 2: [Building Good Datasets:](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch04/ch04.ipynb)\n",
    "    - [A Standardised Approach for Preparing Imaging Data for Machine Learning Tasks in Radiology](https://doi.org/10.1007/978-3-319-94878-2_6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Creating Datasets / Crowdsourcing\n",
    " - [Mindcontrol: A web application for brain segmentation quality control](https://www.sciencedirect.com/science/article/pii/S1053811917302707)\n",
    " - [Combining citizen science and deep learning to amplify expertise in neuroimaging](https://www.biorxiv.org/content/10.1101/363382v1.abstract)\n",
    " \n",
    "- Augmentation tools\n",
    " - [ImgAug](https://github.com/aleju/imgaug)\n",
    " - [Augmentor](https://github.com/mdbloice/Augmentor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "__Why other peoples data?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Most of you taking this class are rightfully excited to learn about new tools and algorithms to analyzing _your_ data. \n",
    "\n",
    "This lecture is a bit of an anomaly and perhaps disappointment because it doesn't cover any algorithms, or tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You might ask, why are we spending so much time on datasets?\n",
    "- You already collected data (sometimes lots of it) that is why you took this class?!\n",
    "\n",
    "... let's see what some other people say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sean Taylor (Research Scientist at Facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This tweet tells us that you shouldn't put too much belief in AI without providing carefully prepared data set. Machine learning methods perform only so good as the data it was trained with. You need a data set that covers all extremes of the fenomena that you want to model.\n",
    "\n",
    "```{figure} figures/data_tweet.jpg\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Realistic thoughts about AI.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/data_tweet.jpg\" style=\"height:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Andrej Karpathy (Director of AI at Tesla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This slide by Andrej Karpathy shows the importance of correct data set in a machine learning project. Typically, you should spend much more time on collecting representative data for your models than building the models. Unfortunately, this is not the case for many PhD projects where the data usually is scarse. Much for the reason that it is really hard to come by the data. You may only have a few beam slots allocated for your experiments and this is the data you have to live with.\n",
    "\n",
    "```{figure} figures/karpathy_slide.jpg\n",
    "---\n",
    "scale: 70%\n",
    "---\n",
    "Time spent on different tasks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/karpathy_slide.jpg\" style=\"height:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kathy Scott (Image Analytics Lead at Planet Labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Yet another tweet that implies that many data scientist actually spend more time on preparing the data than developing new models.  The training is less labor demanding, the computer is doing that part of the job.\n",
    "```{figure} figures/kathy_tweet.png\n",
    "---\n",
    "scale: 70%\n",
    "---\n",
    "The importance to spend sufficient time on data preparation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/kathy_tweet.png\" style=\"height:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data is important\n",
    "It probably [isn't the _new_ oil](https://www.forbes.com/sites/bernardmarr/2018/03/05/heres-why-data-is-not-the-new-oil/), but it forms an essential component for building modern tools today.\n",
    "\n",
    "\n",
    "Testing good algorithms *requires* good data\n",
    "\n",
    " - If you don't know what to expect – *how do you know your algorithm worked*?\n",
    " \n",
    " <br />\n",
    " \n",
    " - If you have dozens of edge cases – *how can you make sure it works on each one*?\n",
    " \n",
    "  <br />\n",
    "  \n",
    " - If a new algorithm is developed every few hours – _how can you be confident they actually work better_? \n",
    "     - facebook's site has a new version multiple times per day and their app every other day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For machine learning, even building algorithms requires good data\n",
    " - If you count cells maybe you can write your own algorithm,\n",
    " \n",
    " - but if you are trying to detect _subtle changes_ in cell structure that indicate cancer you probably can't write a list of simple mathematical rules yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data is reusable\n",
    " \n",
    "- Well organized and structured data is very easy to reuse. \n",
    "- Another project can easily combine your data with their data in order to get even better results.\n",
    "\n",
    "\n",
    "\n",
    "Algorithms are often often only prototypes\n",
    "- messy, \n",
    "- complicated, \n",
    "- poorly written, \n",
    "\n",
    "... especially so if written by students trying to graduate on time.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<center>\n",
    "\n",
    "__Data recycling saves time and improves performance__\n",
    "\n",
    "</center>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Famous Datasets\n",
    "The primary success of datasets has been shown through the most famous datasets collected. \n",
    "\n",
    "Here I show\n",
    "- Two of the most famous general datasets \n",
    "    - MNIST Digits\n",
    "    - ImageNET\n",
    "- and one of the most famous medical datasets.\n",
    "    - BRATS\n",
    "\n",
    "The famous datasets are important for basic network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [MNIST Digits](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Modified NIST (National Institute of Standards and Technology) created a list of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This list is a popular starting point for many machine learning projects. The images are already labeled and are also nicely prepared to about the same size and also very high SNR. These properties makes it a great toy data set for first testing.\n",
    "\n",
    "```{figure} figures/mnist.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "A selection of hand written numbers from the MNIST data base\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/mnist.png\" style=\"height:600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [ImageNet](http://www.image-net.org)\n",
    " - ImageNet is an image database \n",
    "     - organized according to the WordNet hierarchy (currently only the nouns), \n",
    "     - each node of the hierarchy is depicted by hundreds and thousands of images.\n",
    " - 1000 different categories and >1M images.\n",
    " - Not just dog/cat, but wolf vs german shepard, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/A-simplified-WordNet-hierarchy-of-synsets.png\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "Hierarchial structure of the WordNet database.\n",
    "```\n",
    "\n",
    "```{figure} figures/CNN-development.png\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "Error rates for different classifiers on the same data set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr><th>WordNet hierarchy</th><th>Classifier performance ImageNet</th></tr>\n",
    "<tr><td><img src=\"figures/A-simplified-WordNet-hierarchy-of-synsets.png\" style=\"height:300px\"/></td>\n",
    "    <td>\n",
    "        <img src=\"figures/CNN-development.png\" style=\"height:300px\"/>\n",
    "    \n",
    "[CNN architectures](https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "   \n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [BRATS](http://braintumorsegmentation.org) \n",
    "Segmenting Tumors in Multimodal MRI Brain Images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/BRATS_tasks.png\n",
    "---\n",
    "scale: 60%\n",
    "---\n",
    "Images of brain tumors from the BRATS database.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " <img src=\"figures/BRATS_tasks.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What story did these datasets tell?\n",
    "\n",
    "<p>These data sets changed they way people approach the analysis.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "Each of these datasets is very different from images with fewer than 1000 pixels to images with more than 100MPx, but what they have in common is how their analysis has changed.\n",
    "\n",
    "All of these datasets used to be analyzed by domain experts with hand-crafted features. \n",
    "        \n",
    "- A handwriting expert using graph topology to assign images to digits\n",
    "- A computer vision expert using gradients common in faces to identify people in ImageNet\n",
    "- A biomedical engineer using knowledge of different modalities to fuse them together and cluster healthy and tumorous tissue\n",
    "        \n",
    "```{figure} figures/domainexperts.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Domain experts use their experience to analyze data\n",
    "```\n",
    "Starting in the early 2010s, the approaches of deep learning began to improve and become more computationally efficient. With these techniques groups with __absolutely no domain knowledge__ could begin building algorithms and winning contests based on these datasets.\n",
    "\n",
    "```{figure} figures/datascientists.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Data scientist don't have domain specific knowledge, they use available data for the analysis.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Hand-crafted features</th><th>Machine Learning/Deep Learning</th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td>\n",
    "    \n",
    "All of these datasets used to be analyzed by __domain experts__ with hand-crafted features.\n",
    "\n",
    "</td>  \n",
    "<td>\n",
    "Starting in the early 2010s,     \n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"figures/domainexperts.svg\" style=\"height:200px\"/></td>\n",
    "<td><figure><img src=\"figures/datascientists.svg\" style=\"height:100px\"/></figure></td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>\n",
    "        \n",
    "- A handwriting expert using graph topology\n",
    "- A computer vision expert to identify people in ImageNet\n",
    "- A biomedical engineer cluster healthy and tumorous tissue\n",
    "        \n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "- the approaches of deep learning began to improve and become more computationally efficient. \n",
    "- groups with __absolutely no domain knowledge__ could winning contests based on datasets\n",
    "        \n",
    "</td>    \n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So Deep Learning always wins? \n",
    "No, that isn't the point of this lecture. \n",
    "\n",
    "Even if you aren't using deep learning the point of these stories is having \n",
    "- well-labeled, \n",
    "- structured, \n",
    "- and organized datasets \n",
    "\n",
    "makes your problem *a lot more accessible* for other groups and enables a variety of different approaches to be tried. \n",
    "\n",
    "Ultimately it enables better solutions to be made and you to be confident that the solutions are in fact better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to work with someone else’s data\n",
    "\n",
    ">Inherited datasets are like inherited toothbrushes: using them is an act of desperation.\n",
    "\n",
    ">Collecting your own data is a luxury not everyone can afford.\n",
    "\n",
    ">Inherited data are easier to get but harder to trust.\n",
    "\n",
    "[C. Kozyrkov, 2020](https://towardsdatascience.com/how-to-work-with-someone-elses-data-f33485d79ed4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The FAIR principle\n",
    "Open data is a central requirement these days.\n",
    "- __F__indable\n",
    "- __A__ccessible\n",
    "- __I__nteroperable\n",
    "- __R__eusable\n",
    "\n",
    "[Wilkinson et al. 2016](https://doi.org/10.1038/sdata.2016.18)\n",
    "\n",
    "#### PaNOSC\n",
    "[The Photon and Neutron Open Science Cloud (PaNOSC)](https://www.panosc.eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Datasets\n",
    "- Grand-Challenge.org a large number of challenges in the biomedical area\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- [Google Dataset Search](https://datasetsearch.research.google.com/)\n",
    "- [Wikipedia provides a comprehensive list categorized into different topics](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What makes a good dataset?\n",
    "A good data set is characterized by\n",
    "- Large amount\n",
    "- Diversity\n",
    "- Annotations\n",
    "\n",
    "This means that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lots of images\n",
    "\n",
    "- Small datasets can be useful, but here the bigger the better\n",
    " \n",
    " \n",
    "- Particularly if you have:\n",
    "    - Complicated problems\n",
    "    - Very subtle differences (ie a lung tumor looks mostly like normal lung tissue but it is in a place it shouldn't be)\n",
    "    - Class imbalance \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lots of diversity\n",
    " - Is it what data 'in the wild' really looks like?\n",
    " - Lots of different \n",
    "     - Scanners/reconstruction algorithms, \n",
    "     - noise levels, \n",
    "     - illumination types, \n",
    "     - rotation, \n",
    "     - colors, ...\n",
    " - Many examples from different categories \n",
    "     - _if you only have one male with breast cancer it will be hard to generalize exactly what that looks like_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Meaningful labels\n",
    " - Clear task or question\n",
    " - Unambiguous (would multiple different labelers come to the same conclusion)\n",
    " - Able to be derived from the image alone \n",
    "    - _A label that someone cannot afford insurance is interesting but it would be nearly impossible to determine that from an X-ray of their lungs_\n",
    " - Quantiative!\n",
    " - Non-obvious \n",
    "     - _A label saying an image is bright is not a helpful label because you could look at the histogram and say that_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Purpose of different types of Datasets\n",
    "- Classification\n",
    "- Regression\n",
    "- Segmentation\n",
    "- Detection\n",
    "- Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/classificationCD.pdf\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Classification example with cats and dogs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column23\">\n",
    "      \n",
    "- Taking an image and putting it into a category\n",
    "- Each image should have exactly one category\n",
    "- The categories should be non-ordered\n",
    "- Example: \n",
    " - Cat vs Dog\n",
    " - Cancer vs Healthy\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "    <img src=\"figures/classificationCD.svg\" style=\"height:400px\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In classification you want to observe an image an quickly provide it with a category. Like in the MNIST example which is designed for recognition of handwritten numbers. Each image has a label telling which number it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(5, 5))\n",
    "for c_ax, c_img, c_label in zip(m_axs.flatten(), img, label):\n",
    "    c_ax.imshow(c_img, cmap='gray')\n",
    "    c_ax.set_title(c_label)\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Regression almost looks like classification at first sight. You still want to put a number related to the image content. But here it is not strictly bound to the provided categories but rather estimating a value which can be found on the regression line fitted to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Taking an image and predicting one (or more) decimal values\n",
    "\n",
    "\n",
    "- Examples: \n",
    " - Value of a house from the picture taken by owner\n",
    " - Risk of hurricane from satellite image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression example [Age from X-Rays](https://www.kaggle.com/kmader/rsna-bone-age) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This dataset contains a collection of X-ray radiographs of hands. The purpose of the data is to estimate the age of a child based on the radiograph. This can be done using a regression model.\n",
    "\n",
    "```{figure} figures/bone_age.png\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "A collection of X-ray images from children at different ages.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"figures/bone_age.png\" style=\"height:600px\">\n",
    "</center>\n",
    "\n",
    "[More details](https://www.kaggle.com/kmader/attention-on-pretrained-vgg16-for-bone-age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column23\">\n",
    "      \n",
    "- Taking an image and predicting one (or more) values for each pixel\n",
    "- Every pixel needs a label (and a pixel cannot have multiple labels)\n",
    "- Typically limited to a few (less than 20) different types of objects\n",
    "\n",
    "**Examples:**     \n",
    "- Where a tumor is from an image of the lungs?\n",
    "- Where streets are from satellite images of a neighborhood?\n",
    "- Where are the cats and dogs?\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column13\">\n",
    "     <img src=\"figures/segmentationCD.svg\" style=\"height:600px\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Segmentation example: Nuclei in Microscope Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/dsb_sample/slide.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Sample with cells.\n",
    "```\n",
    "\n",
    "```{figure} figures/dsb_sample/labels.png\n",
    "---\n",
    "scale: 50%\n",
    "---\n",
    "Labelled cells.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table>\n",
    "     <tr>\n",
    "        <th>\n",
    "            <center>Image</center>\n",
    "        </th>\n",
    "        <th>\n",
    "            <center>Labelled</center>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"figures/dsb_sample/slide.png\" style=\"height:600px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figures/dsb_sample/labels.png\" style=\"height:600px\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "[More details on Kaggle](https://www.kaggle.com/c/data-science-bowl-2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Detection is a combination of segmenation and classification in the sense that the location and extents of a feature is determined and is also categorized into some class. The extents don't have to be very precise, it is often a bounding box or a convex hull. This coarseness is sufficient for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - Taking an image and predicting where and which type of objects appear\n",
    " - Generally bounding box rather than specific pixels\n",
    " - Multiple objects can overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/lung_opacity.png\n",
    "---\n",
    "scale: 60%\n",
    "---\n",
    "Radiographs to detect opaque regions in X-Rays\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detection example: Opaque Regions in X-Rays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example the task is to detect opaque regions in lung X-ray images to provide a first indication for the physician who should make a diagnosis from the images. The used algorithm marks rectangles on region that are too opaque to be healthy.\n",
    "\n",
    "```{figure} figures/lung_opacity.png\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "Critical regions detected in lung radiographs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/lung_opacity.png\" style=\"height:600px\" />\n",
    "\n",
    "[More details on Kaggle](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other\n",
    " - Unlimited possibilities [here](https://junyanz.github.io/CycleGAN/)\n",
    " - Horses to Zebras \n",
    "\n",
    "### Image Enhancement \n",
    "  - Denoising [Learning to See in the Dark](http://cchen156.web.engr.illinois.edu/SID.html)\n",
    "  - [Super-resolution](https://data.vision.ee.ethz.ch/cvl/DIV2K/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building your own data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, we arrive at your data! As you already have seen, it is a time consuming and labor intense task to collect and prepare data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Very time consuming\n",
    "- Not a lot of great tools\n",
    "- Very problem specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is however important to have well-organized data for the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Code-free\n",
    "\n",
    "### Classification\n",
    "- Organize images into folders\n",
    "\n",
    "### Regression\n",
    "- Create an excel file (first column image name, next columns to regress)\n",
    "\n",
    "### Segmentation / Object Detection\n",
    "- Take [FIJI](http://fiji.sc/) or any paint application and manually draw region to be identified and save it as a grayscale image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Software for data labelling\n",
    "### Free tools\n",
    " - [Classification / Segmentation](https://github.com/Labelbox/Labelbox)\n",
    " - [Classification/ Object Detection](http://labelme.csail.mit.edu/Release3.0/)\n",
    " - [Classification](https://github.com/janfreyberg/superintendent) [(demo)](https://www.youtube.com/watch?v=fMg0mPYiEx0)\n",
    " - [Classification/ Detection](https://github.com/chestrays/jupyanno) \n",
    " - [Classification (Tinder for Brain MRI)](https://braindr.us/#/)\n",
    " \n",
    "### Commercial Approaches\n",
    " - https://www.figure-eight.com/\n",
    " - MightyAI / Spare5: https://mighty.ai/ https://app.spare5.com/fives/sign_in\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: annotation of spots\n",
    "Spots are outliers in radiography and very annoying when the images are used for tomography. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/markedspots.pdf\n",
    "---\n",
    "---\n",
    "Annotation of spot in a neutron radiograph.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/markedspots.svg\" style=\"width:50%\">\n",
    "\n",
    "- Image size 2048x2048\n",
    "- Tools\n",
    "    - Bitmap painting application \n",
    "    - Drawing tablet\n",
    "- Time to markup _8h_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A further way to increase training data is to build a model of the features you want to train on. This approach has the advantage that you know where to look for the features which means the tedious annotation task is reduced to a minimum. The work rather lies in building a relieable model that should reflect the characteristics of features you want to segments. Once a valid model is built, it is easy to generate masses of data under variuos conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations can be done using:\n",
    "- Geometric models\n",
    "- Template models\n",
    "- Physical models\n",
    "\n",
    "Both augmented and simulated data should be combined with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Another way to enhance or expand your dataset is to use simulations\n",
    "- already incorporate realistic data (game engines, 3D rendering, physics models)\n",
    "- 100% accurate ground truth (original models)\n",
    "- unlimited, risk-free playability (driving cars in the world is more dangerous)\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- [P. Fuchs et al. - Generating Meaningful Synthetic Ground Truth for\n",
    "Pore Detection in Cast Aluminum Parts, iCT 2019, Padova](https://pdfs.semanticscholar.org/30a1/ba9142b9c3b755da2bff7d93d704494fdaed.pdf)\n",
    "- [Playing for Data: Ground Truth from Computer Games](https://download.visinf.tu-darmstadt.de/data/from_games/)\n",
    "- [Self-driving car](https://pythonprogramming.net/self-driving-car-neural-network-training-data-python-plays-gta-v/)\n",
    "- [Learning from simulated data](https://towardsdatascience.com/learning-from-simulated-data-ff4be63ac89c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset Problems\n",
    "Some of the issues which can come up with datasets are \n",
    "- imbalance\n",
    "- too few examples\n",
    "- too homogenous \n",
    "- and other possible problems\n",
    "\n",
    "These lead to problems with the algorithms built on top of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Working with single sided data will bias the model towards this kind of data. This is a reason for the need to include as many corner cases as possible in the data. Biasing can easily happen when you have too few data to provide a statistically well founded training.\n",
    "\n",
    "The gorilla example may sound fun but it can also upset people and in some cases the wrong descision can even cause inrepairable damage. Google's quick fix to the problem was to remove the gorilla category from their classifyer. This approach may work for a trivial service like picture categorization tool, but yet again what if it is an essential category for the model?\n",
    "\n",
    "```{figure} figures/google-racist-gorilla-doctored-tweet.png\n",
    "---\n",
    "scale: 60%\n",
    "---\n",
    "Mistakes that can happen due bias caused by insufficent training data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/google-racist-gorilla-doctored-tweet.png\" style=\"height:500px\" />\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<font size=5>[The solution was to remove Gorilla from the category](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A better solution to avoid biasing\n",
    "\n",
    "__Use training sets with more diverse people__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The better solution to avoid biasing mistakes is to use a large data base with more variations one example is the [IBM Diverse Face Dataset](https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/). This face dataset not only provides great variation in people but also adds features to categorize the pictures even further. The figure below shows some samples from the face dataset with categories like:\n",
    "- Accessories like eyeglases and hats\n",
    "- Different hair styles\n",
    "- Face shapes\n",
    "- Face expressions\n",
    "\n",
    "```{figure} figures/celeb_dataset.png\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "Use a database with more diverse people to avoid biasing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/celeb_dataset.png\" style=\"height:500px\">\n",
    "\n",
    "<a href=\"https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/\">IBM Diverse Face Dataset</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the prevoius example with face pictures we started to look into categories of pictures. These pictures were provided with labels bescribing the picture content. The next dataset we will look at is the MNIST data set, which we already have seen a couple of times in this lecture.\n",
    "\n",
    "In the example below we have extracted the numbers 1,2, and 3. The histogram to the right shows the distribution of the numbers in the extracted data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(img, label), _ = mnist.load_data()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "d_subset = np.where(np.in1d(label, [1, 2, 3]))[0]\n",
    "\n",
    "# Visualization\n",
    "ax1.imshow(montage2d(img[d_subset[:64]]), cmap='gray'), ax1.set_title('Images'), ax1.axis('off')\n",
    "ax2.hist(label[d_subset[:64]], np.arange(11)),          ax2.set_title('Digit Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Machine learning methods require a lot of training data to be able to build good models that are able to detect the features they are intended to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Different types of limited data_:\n",
    "- Few data points or limited amounts of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is very often the case in neutron imaging. The number of images collected during an experiment session is often very low due to the long experiment duration and limited amount of beam time. This makes it hard to develop segmentation and analysis methods for single experiments. The few data points problem can partly be overcome by using data from previous experiment with similar characteristics. The ability to recycle data depends on what you want to detect in the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Unbalanced data means that the ratio between the data points with features you want detect and the total number data points is several orders of magnitude. E.g roots in a volume like the example we will look at later in this lecture. There is even a risk that the distribution of the wanted features is overlapped by the dominating background distribution.\n",
    "\n",
    "```{figure} figures/classunbalance.pdf\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "Two cases of unblanaced data; (a) the classes are well separated and the feature class is clearly visible in the tail distribution of the background and (b) the feature class is embeded in the background making it hard to detect.\n",
    "```\n",
    "\n",
    "Case (a) can most likely be segmented using one of the many histogram based thresholding methods proposed in literature. Case (b) is much harder to segment as the target features have similar gray levels as the background. This case requires additional information to make segmentation posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure><img src=\"figures/classunbalance.svg\" style=\"width:70%\"></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Little or missing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A complete set of training data contains both input data and labelled data. The input data is easy to obtain, it is the images you measured during your experiment. The labelled data is harder to get as it is a kind of chicken and egg problem. In particular, if your experiment data is limited. In that case, you would have to mark-up most of the available data to obtain the labeled data. Which doesn't make sense because \n",
    "- then you'd already solved the task before even starting to train your segmentation algorithm. \n",
    "- An algorithm based on learning doesn't improve the results, it only make it easier to handle large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Obtaining more experiment data is mostly relatively hard, \n",
    "\n",
    "- Time in the lab is limited. \n",
    "- Sample preparation is expensive.\n",
    "- The number of specimens is limited. \n",
    "\n",
    "Still, many supervised analysis methods require large data sets to perform reliably. A method to improve this situation is to use data augmentation. This means that you take the existing data and distorts it using different transformations or add features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Most groups have too little well-labeled data and labeling new examples can be very expensive. \n",
    "- Additionally there might not be very many cases of specific classes. \n",
    "- In medicine this is particularly problematic, because some diseases might only happen a few times in a given hospital and you still want to be able to recognize the disease and not that particular person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical augmentation operations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "### Transformations\n",
    "- Shift\n",
    "- Zoom\n",
    "- Rotation\n",
    "- Intensity\n",
    " - Normalization\n",
    " - Scaling\n",
    "- Color\n",
    "- Shear\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    \n",
    "### Further modifications\n",
    "- Add noise\n",
    "- Blurring\n",
    "      \n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some augmentation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The figure below shows some examples of augmentations of the same image. You can also add noise and modulate the image intensity to increase the variations further.\n",
    "```{figure} figures/Augmentations.pdf\n",
    "---\n",
    "scale: 100%\n",
    "---\n",
    "A retinal image modified using different augmentation techniques (source: https://drive.grand-challenge.org/DRIVE/) prepared by Gian Guido Parenza.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure>\n",
    "<img src=\"figures/Augmentations.svg\" style=\"height:500px\">\n",
    "<figcaption>\n",
    "    \n",
    "Retial images from [DRIVE](https://drive.grand-challenge.org/DRIVE/) prepared by Gian Guido Parenza.\n",
    "    \n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations of augmentation\n",
    "\n",
    "- What transformations are normal in the images?\n",
    " - CT images usually do not get flipped (the head is always on the top)\n",
    " - The values in CT images have a physical meaning (Hounsfield unit), <br /> $\\rightarrow$ scaling them changes the image\n",
    " \n",
    " \n",
    "- How much distortion is too much? \n",
    " - Can you still recognize the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras ImageDataGenerator\n",
    "Help page of the data generator\n",
    "```\n",
    "ImageDataGenerator(\n",
    "    ['featurewise_center=False', 'samplewise_center=False', 'featurewise_std_normalization=False', 'samplewise_std_normalization=False', 'zca_whitening=False', 'zca_epsilon=1e-06', 'rotation_range=0.0', 'width_shift_range=0.0', 'height_shift_range=0.0', 'shear_range=0.0', 'zoom_range=0.0', 'channel_shift_range=0.0', \"fill_mode='nearest'\", 'cval=0.0', 'horizontal_flip=False', 'vertical_flip=False', 'rescale=None', 'preprocessing_function=None', 'data_format=None'],\n",
    ")\n",
    "Docstring:     \n",
    "Generate minibatches of image data with real-time data augmentation.\n",
    "\n",
    "# Arguments\n",
    "    featurewise_center: set input mean to 0 over the dataset.\n",
    "    samplewise_center: set each sample mean to 0.\n",
    "    featurewise_std_normalization: divide inputs by std of the dataset.\n",
    "    samplewise_std_normalization: divide each input by its std.\n",
    "    zca_whitening: apply ZCA whitening.\n",
    "    zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "    rotation_range: degrees (0 to 180).\n",
    "    width_shift_range: fraction of total width, if < 1, or pixels if >= 1.\n",
    "    height_shift_range: fraction of total height, if < 1, or pixels if >= 1.\n",
    "    shear_range: shear intensity (shear angle in degrees).\n",
    "    zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n",
    "        in the range [1-z, 1+z]. A sequence of two can be passed instead\n",
    "        to select this range.\n",
    "    channel_shift_range: shift range for each channel.\n",
    "    fill_mode: points outside the boundaries are filled according to the\n",
    "        given mode ('constant', 'nearest', 'reflect' or 'wrap'). Default\n",
    "        is 'nearest'.\n",
    "        Points outside the boundaries of the input are filled according to the given mode:\n",
    "            'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "            'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "            'reflect':  abcddcba|abcd|dcbaabcd\n",
    "            'wrap':  abcdabcd|abcd|abcdabcd\n",
    "    cval: value used for points outside the boundaries when fill_mode is\n",
    "        'constant'. Default is 0.\n",
    "    horizontal_flip: whether to randomly flip images horizontally.\n",
    "    vertical_flip: whether to randomly flip images vertically.\n",
    "    rescale: rescaling factor. If None or 0, no rescaling is applied,\n",
    "        otherwise we multiply the data by the value provided. This is\n",
    "        applied after the `preprocessing_function` (if any provided)\n",
    "        but before any other transformation.\n",
    "    preprocessing_function: function that will be implied on each input.\n",
    "        The function will run before any other modification on it.\n",
    "        The function should take one argument:\n",
    "        one image (Numpy tensor with rank 3),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Keras ImageDataGenerator example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are quite many degrees of freedom to use the ImageDataGenerator. The generator is given all boundary condition at initialization time. Below you see an example of how it can be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_aug = ImageDataGenerator(\n",
    "    featurewise_center  = False,\n",
    "    samplewise_center   = False,\n",
    "    zca_whitening       = False,\n",
    "    zca_epsilon         = 1e-06,\n",
    "    rotation_range      = 30.0,\n",
    "    width_shift_range   = 0.25,\n",
    "    height_shift_range  = 0.25,\n",
    "    shear_range         = 0.25,\n",
    "    zoom_range          = 0.5,\n",
    "    fill_mode           = 'nearest',\n",
    "    horizontal_flip     = False,\n",
    "    vertical_flip       = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Augmenting MNIST images\n",
    "Even something as simple as labeling digits can be very time consuming (maybe 1-2 per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(img, label), _ = mnist.load_data(); img = np.expand_dims(img, -1)\n",
    "fig, m_axs = plt.subplots(4, 10, figsize=(14, 7))\n",
    "# setup augmentation\n",
    "img_aug.fit(img)\n",
    "real_aug = img_aug.flow(img[:10], label[:10], shuffle=False)\n",
    "for c_axs, do_augmentation in zip(m_axs, [False, True, True, True]):\n",
    "    if do_augmentation:\n",
    "        img_batch, label_batch = next(real_aug)\n",
    "    else:\n",
    "        img_batch, label_batch = img, label\n",
    "    for c_ax, c_img, c_label in zip(c_axs, img_batch, label_batch):\n",
    "        c_ax.imshow(c_img[:, :, 0], cmap='gray', vmin=0, vmax=255)\n",
    "        c_ax.set_title('{}\\n{}'.format( c_label, 'aug' if do_augmentation else '')), c_ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A larger open data set\n",
    "We can use a more exciting dataset to try some of the other features in augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here are some examples from the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset\n",
    "\n",
    "<img src=\"figures/CIFAR10-examples.png\" style=\"height:500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Augmenting CIFAR10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(img, label), _ = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "img_aug = ImageDataGenerator(\n",
    "    featurewise_center  = True,\n",
    "    samplewise_center   = False,\n",
    "    zca_whitening       = False,\n",
    "    zca_epsilon         = 1e-06,\n",
    "    rotation_range      = 30.0,\n",
    "    width_shift_range   = 0.25,\n",
    "    height_shift_range  = 0.25,\n",
    "    channel_shift_range = 0.25,\n",
    "    shear_range         = 0.25,\n",
    "    zoom_range          = 1,\n",
    "    fill_mode           = 'reflect',\n",
    "    horizontal_flip     = True,\n",
    "    vertical_flip       = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running the CIFAR augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 10, figsize=(18, 8))\n",
    "# setup augmentation\n",
    "img_aug.fit(img)\n",
    "real_aug = img_aug.flow(img[:10], label[:10], shuffle=False)\n",
    "for c_axs, do_augmentation in zip(m_axs, [False, True, True, True]):\n",
    "    if do_augmentation:\n",
    "        img_batch, label_batch = next(real_aug)\n",
    "        img_batch -= img_batch.min()\n",
    "        img_batch = np.clip(img_batch/img_batch.max() *\n",
    "                            255, 0, 255).astype('uint8')\n",
    "    else:\n",
    "        img_batch, label_batch = img, label\n",
    "    for c_ax, c_img, c_label in zip(c_axs, img_batch, label_batch):\n",
    "        c_ax.imshow(c_img)\n",
    "        c_ax.set_title('{}\\n{}'.format(\n",
    "            c_label[0], 'aug' if do_augmentation else ''))\n",
    "        c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baselines\n",
    "- A baseline is \n",
    "    - a simple, \n",
    "    - easily implemented and understood model \n",
    "    - that illustrates the problem \n",
    "    - and the 'worst-case scenario' for a model that learns nothing (some models will do worse, but these are especially useless).\n",
    "- Why is this important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baseline model example\n",
    "I have a a model that is >99% accurate for predicting breast cancer:\n",
    "\n",
    "$$ \\textrm{DoIHaveBreastCancer}(\\textrm{Age}, \\textrm{Weight}, \\textrm{Race}) = \\textrm{No!} $$\n",
    "\n",
    "<div class=\"alert alert-box alert-danger\">\n",
    "Breast Cancer incidence is $\\approx$ 89 of 100,000 women (0.09%) ...\n",
    "<br/><br/>\n",
    "...so always saying <b>no</b> has an accuracy of <b>99.91%</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's train the dummy classifier with some values related to healthy and cancer sick patients. Measurements values 0,1, and 2 are healthy while the value 3 has cancer. We train the classifyer with the strategy that the most frequent class will predict the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "dc.fit([0, 1, 2, 3], \n",
    "       ['Healthy', 'Healthy', 'Healthy', 'Cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Testing the outcome of the classifyer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for idx in [0,1,3,100] :\n",
    "    print('Prediction for {0} is {1}'.format(idx,dc.predict([idx])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "With these few lines we test what happens when we provide some numbers to the classifyer. The numbers are \n",
    "- 0 and 1, which are expected to be healthy\n",
    "- 3 , which has cancer\n",
    "- 100, unknown to the model\n",
    "\n",
    "So, the classifyer tells us that all values are from healthy patients... not really good! The reason is that it was told to tell us the category of the majority, which is that the patient is healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Try dummy classifier on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The previous basic problem showed us how the dummy classifier work. Now we want to use it with the handwritten numbers in the MNIST dataset. The first step is to load the data and check how the distribution of numbers in the data set using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(8, 8)); m_axs= m_axs.ravel()\n",
    "m_axs[0].hist(label[:24], np.arange(11)), m_axs[0].set_title('Digit Distribution')\n",
    "\n",
    "for i, c_ax in enumerate(m_axs[1:]):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title(label[i]); c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's train the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we want to train the model with our data. Once again we use the _most frequent_ model. The training is done in the first 24 images in the data set. The fitting requires that we provide the images with numbers and their associated labels telling the model how to interpret the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "dc.fit(img[:24], label[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__A basic test__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the basic test, we provide the first ten images and hope to get predictions which numbers they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dc.predict(img[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test on the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's see how good these predictions really are by showing the images along with their labels and the prediction of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "\n",
    "    prediction = dc.predict(img[i])[0]\n",
    "    \n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i],prediction), color='green' if prediction == label[i] else 'red'), c_ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ... why are all predictions = 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The result of the basic classifyer was quite disapointing. It told us that all ten images contained the number '1'. Now, why is that?\n",
    "\n",
    "This can be explained by looking at the label histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(label[:24], np.arange(11)); plt.title('Frequency of numbers in the training data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here, we see that there are most '1's in the training data. We have been using the _most frequent_ model for training the classifyer therefore the response '1' is the only answer the model can give us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest Neighbor\n",
    "A better baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This isn't a machine learning class and so we won't dive deeply into other methods, but nearest neighbor is often a very good baseline (that is also very easy to understand). You basically take the element from the original set that is closest to the image you show. \n",
    "\n",
    "```{figure} figures/Russ_fig12_58.png\n",
    "---\n",
    "scale: 80%\n",
    "---\n",
    "Examples of the k-nearest neighbors classifyer ([Figure from J. Russ, Image Processing Handbook](https://www.crcpress.com/The-Image-Processing-Handbook/Russ-Neal/p/book/9781138747494)).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/Russ_fig12_58.png\" style=\"height:400px\">\n",
    "\n",
    "<a href=\"https://www.crcpress.com/The-Image-Processing-Handbook/Russ-Neal/p/book/9781138747494\"><font size=2>Figure from J. Russ, Image Processing Handbook</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can make the method more robust by using more than one nearest neighbor (hence K nearest neighbors), but that we will cover later in the supervised methods lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's load the data again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's come back to the MNIST numbers again. This time, we will try the k-nearest neighbors as baseline and see if we can get a better result than with the dummy classifyer with majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "m_axs[0, 0].hist(label[:24], np.arange(11))\n",
    "m_axs[0, 0].set_title('Digit Distribution')\n",
    "for i, c_ax in enumerate(m_axs.flatten()[1:]):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title(label[i]); c_ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training k-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The training of the k-nearest neigbors consists of filling feature vectors into the model and assign each vector to a class.\n",
    "\n",
    "But images are not vectors... so what we do is to rearrange the $N\\times{}M$ images into a vector with the dimensions $M\\cdot{}N\\times{}1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh_class = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "N = 24\n",
    "neigh_class.fit(img[:N].reshape((N, -1)), label[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predict on a few images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The prediction of which class an image belongs to is done by reshaping the input image into a vector in the same manner as for the training data. Now we will compare the input vector $u$ to all the vectors in the trained model $v_i$ by computing the Euclidean distance between the vectors. This can easily be done by the inner product of the two vectors:\n",
    "\n",
    "$$D_i=(v_i-u)^T \\cdot{} (v_i-u) = scalar$$\n",
    "\n",
    "The class is chosen by the model vector that is closest to the input vector, i.e. having the smallest $D_i$. This calculations are done for you as a black box in the ```KNeighborsClassifier```, you only have to reshape the images into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "neigh_class.predict(img[0:10].reshape((10, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compare predictions with the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    \n",
    "    prediction = neigh_class.predict(img[i].reshape((1, -1)))[0]\n",
    "    \n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i],prediction), color='green' if prediction == label[i] else 'red')\n",
    "    c_ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 100% correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 100% for a baseline !!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Wow the model works really really well, it got every example perfectly. \n",
    "\n",
    "What we did here (a common mistake) was evaluate on the same data we 'trained' on which means the model just correctly recalled each example. This is natural as there is always an image that gives the distance $D_i=0$. \n",
    "\n",
    "Now, if we try it on new images we can see the performance drop but still a somewhat reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten(), 25):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    prediction = neigh_class.predict(img[i].reshape((1, -1)))[0]; \n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i],prediction), color='green' if prediction == label[i] else 'red')\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-danger\">\n",
    "A classic mistake: testing and training with the same data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How good is good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From the previous example, we saw that the classify doesn't really reach the 100% accuracy on unseen data, but rather makes a mistake here or there. Therefore we need to quantify how good it really is to be able to compare the results with other algorithms. We will cover more tools later in the class but now we will show the accuracy and the confusion matrix for our simple baseline model to evaluate how well it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The confusion matrix is a kind of histogram where you count the number of predicted occurances for each actual label. This gives us an idea about the classifyier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We show which cases were most frequently confused \n",
    "    \n",
    "|n=165|Predicted TRUE|Predicted FALSE|\n",
    "|:---:|:---:|:---:|\n",
    "|__Actual TRUE__|_50_|10|\n",
    "|__Actual FALSE__|5|_100_|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is only a simple matrix for the two cases _true_ and _false_. The matrix does however grow with the number of classes in the data set. It is always a square matrix as we have the same number of actual classes as we have predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix for the MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We saw that the k-nearest neighbors did a couple of missclassifications on the unseen test data. Now is the question how many mistakes it really does and how many correct labels it assigned. If compute the confusion matrix for this example, we will get a 10x10 matrix i.e. one for each class in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Stolen from: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "pred_values = neigh_class.predict(img[24:].reshape((-1, 28*28)))\n",
    "ax1 = print_confusion_matrix(confusion_matrix(label[24:], pred_values), class_names=range(10))\n",
    "ax1.set_title('Accuracy: {:2.2%}'.format(accuracy_score(label[24:], pred_values)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this confusion matrix we see that some numbers are easier to classify than others. Some examples are:\n",
    "- The '0' seems to be hard to confuse with other numers. \n",
    "- Many images from all categories are falsely assigned to the '1'\n",
    "- The number '4' is more probable to be assigned a label '9' than the '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This experiment was done with a very limited training data set. You can experiment with more neighbors and more training data to see what improvement that brings. In all, there are 60000 images with digits in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- The importance of good data\n",
    "- What is good data\n",
    "- Preparing data\n",
    "- Famous data sets\n",
    "- Augmentation\n",
    "    - Transformations for increase the data\n",
    "- Baseline algorithms\n",
    "    - What is it?\n",
    "    - Why do we need it?\n",
    "    - How good is our baseline algorithm?\n",
    "    - The confusion matrix"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "footer": "March 3, 2022 - ETH 227-0966-00L: Quantitative Big Imaging/Ground truth",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
